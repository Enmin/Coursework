{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "hw7-sequences-b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "OmeBqkWx8_xg"
      },
      "source": [
        "### Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "b76a158a-36d3-49fb-bd22-95a5e899d8b1",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "VU2NVz9G8_xo"
      },
      "source": [
        "®[4] Review L16 and Ch. 8.3 in the book.\n",
        "\n",
        "Given a long sequence \"_deep learning is fun_\", manually generate two mini-batches by the **random partitioning**. Set the batch size to 2 and the number of steps per subsequence to 3. (No coding required)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM03LJA__VRy"
      },
      "source": [
        "Random indices: [6, 3, 9, 12, 0]\n",
        "\n",
        "Start indices: 0, Batch 1: ['rni', 'lea'] ['nin', 'ear']\n",
        "\n",
        "Start indices: 2, Batch 2: ['ng ', 'is '] ['g i', 's f']\n",
        "\n",
        "The offset I choose for this is 2. \n",
        "The length of each input subsequence and output label will be 3 and the number of subsequences within each batch is 2. The elements in the first list of batch 1 are Xs (input) and the elements in the second list of batch 1 are Ys (labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNtKPSh-7YL",
        "outputId": "e8783a34-484b-4b1d-a570-9171c5a4d9d7"
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos:pos + num_steps]\n",
        "    print(initial_indices)\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
        "        print(i)\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        print(X, Y)\n",
        "        # yield tf.constant(X), tf.constant(Y)\n",
        "seq_data_iter_random(\"deep learning is fun\", 2, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 3, 9, 12, 0]\n",
            "0\n",
            "['rni', 'lea'] ['nin', 'ear']\n",
            "2\n",
            "['ng ', 'is '] ['g i', 's f']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "aiDuitmu8_xp"
      },
      "source": [
        "### Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "227e0840-c03f-4b10-8746-707865caf4a6",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "U_1e-Isx8_xp"
      },
      "source": [
        "®[4] Given a long sequence \"_deep learning is fun_\", manually generate two mini-batches by the **sequential partitioning**. Set the batch size to 2 and the number of steps per subsequence to 3. (No coding required)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVTyWSOBF9Qy"
      },
      "source": [
        "Batch 1: [dee, lea], [eep, ear]\n",
        "\n",
        "Batch 2: [p l, rni], [ le, nin]\n",
        "\n",
        "The batch elements are adjacent between the input in two batches. The offset I choose for this is 0. The length of each input subsequence and output label will be 3 and the number of subsequences within each batch is 2. The elements in the first list of batch 1 are Xs (input) and the elements in the second list of batch 1 are Ys (labels)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "JZO_aTgP8_xq"
      },
      "source": [
        "### Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "547cfdf1-bddf-4c15-a7e3-e500841ae55d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "JXuA5r778_xr"
      },
      "source": [
        "®[4] We might encounter situations where some tokens carry no pertinent observation. For instance, when parsing a web page there might be auxiliary HTML code that is irrelevant for the purpose of assessing the sentiment conveyed on the page. We would like to have some mechanism for _skipping_ such tokens in the latent state representation. Which gate(s) in LSTM can help us do that, and how? (Explain how by referring to the equations in L16.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TykuPFSwJrBq"
      },
      "source": [
        "We should use input gate and output gate. Since the input gate controls how much information we will input to our memory cell from our current input data. Since HTML code is our input via $\\hat{C}_t$, we need to use input state to take out these useless information. Second term in $C_t = F_t * C_{t-1} + I_t * \\hat{C}_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "dP87hwXs8_xs"
      },
      "source": [
        "### Problem 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "ba3ac6af-e92f-4616-baed-bcb88b884237",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "Ald76nQV8_xt"
      },
      "source": [
        "®[4] We might encounter situations where there is a logical break between parts of a sequence. For instance, there might be a transition between chapters in a book, or a transition between a bear and a bull market for securities. In this case, it would be nice to have a means of _resetting_ our internal state representation. Which gate(s) in LSTM can help us do that, and how? (Explain how by referring to the equations in L16.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgZ3Tl_uLDJb"
      },
      "source": [
        "We should use the Forget Gate and output gate. Since Forget Gate controls how much information from previous memory via $C_{t-1})$ we want to parse into our current memory cell $C_t$, and the transition means we need to take out some of the memory from previous memory cell, we should use Forget Gate. First term in $C_t = F_t * C_{t-1} + I_t * \\hat{C}_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "PVcfFqU08_xv"
      },
      "source": [
        "### Problem 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "667d0993-68cb-4199-afdb-e846f3266762",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "m01auqVI8_xw"
      },
      "source": [
        "During the lecture, we discussed this graphical representation for Recurrent Neural networks (RNNs):\n",
        "![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-c708e21f-99b8-4b13-9fd9-152900868451.png?alt=media&token=9f701240-fac3-47a7-b111-1038c1129fca)\n",
        "[Image credit](https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "bc244d1b-aec7-4f1e-abd8-77b9895788f1",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "IGnoI4ma8_xx"
      },
      "source": [
        "We also discussed this algebraic RNN representation: \n",
        "\n",
        "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$:  A minibatch of size n, containing sequence examples at time t.\n",
        "$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$, $\\mathbf{H}_0=\\mathbf{0}$: The hidden variable of time step $t$.\n",
        "\n",
        "\n",
        "$$\n",
        "\\displaystyle ﻿\\displaystyle ﻿\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h).\n",
        "$$\n",
        "\n",
        "$\\mathbf{O}\\in\\mathbb{R}^{n\\times{q}}$: The output at time step $t$. \n",
        "\n",
        "$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$(8.4.6)\n",
        "\n",
        "( some notations are different from the figure above.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "4c304f1c-15af-4108-9cfe-3a5c00b83d23",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "V4c1KLXQ8_xx"
      },
      "source": [
        "Understanding how these two representations are related is important.  If you examine it carefully, you will conclude the algebraic equations describe a more detailed story than the RNN diagram.  The story they tell is that of an RNN acting on a minibatch containing $n$ training examples. Processing an entire mini-batch where each sample of length $k$ requires $k$  distinct steps.  First $X_1$, then $X_2$, and so on until $X_k$ is calculated.\n",
        "\n",
        "A minibatch {[a1, a2], [b1,b2], [c1,c2]} containing 3 sequences, each with 2 elements, is fed in as follows,\n",
        "\n",
        "$X_1$ = [a1,b1,c1] \n",
        "$X_2$ = [a2,b2,c3] \n",
        "\n",
        "$d$ is the dimension of each element in a sequence (e.g., a1)\n",
        "\n",
        "**Important Concept:** With CNN models, SGD updates are found after applying the model to each image (sample) in the minibatch, there is no intra-sample stepping.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "bcaac1d7-cb43-41d3-af05-f1797ad3ba43",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "IjxtviSe8_xy"
      },
      "source": [
        "®[5] Task: Understanding batching for RNNs\n",
        "Please answer the following questions in the cell below:\n",
        "1. How many parameters define the hidden state?\n",
        "2. Why is the dimension of $H_t$ n x h,  and not 1 x h or h x 1?\n",
        "3. In order for $H_t$ to be learned in sequence, how should the samples in sequential mini-batches be organized?  Create a two minibatch example.\n",
        "4. Explain why `generate_text` in hw6-sequence-b had to rebuild the model before it could be used for sequence generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhLdZ95yyKuL"
      },
      "source": [
        "1. d * h + h^2 + h parameters from formula.\n",
        "2. Since there are n different individual hidden states.\n",
        "3. [123, lea], [456, rni]\n",
        "4. Since the input length might be different when used for sequence generation. If we change the input size, we need to rebuild the model to adapt to that input size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "I8NroWLa8_xz"
      },
      "source": [
        "### Problem 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "b3b1663c-f7e4-4555-990c-c98856ee357d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "wILPoGXb8_xz"
      },
      "source": [
        "The following questions are based on:\n",
        "[https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "9a239b36-0128-4284-8d25-d16688423c9b",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "C6CO4L3m8_xz"
      },
      "source": [
        "The example Spanish output of our preprocessed sentence looks like this\n",
        "```python\n",
        ">>> sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        ">>> print(preprocess_sentence(sp_sentence).encode('utf-8'))\n",
        "\n",
        "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "prismiaParentId": "9a239b36-0128-4284-8d25-d16688423c9b",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "dW84_kZO8_x0"
      },
      "source": [
        ">>> sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        ">>> print(preprocess_sentence(sp_sentence).encode('utf-8'))\n",
        "\n",
        "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "6b246c8f-66b5-4018-9b3f-4d2a79e00947",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "Y5uo6C9n8_x1"
      },
      "source": [
        "®[2] `\\xc2\\xbf` looks weird, is it a bug? Where did it come from and how will the model handle it?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hL_Ch2Im6_I"
      },
      "source": [
        "\"\\xc2\\xbf\" is the way that \"utf-8\" encode the symbol \"¿\". When creating dataset, the code are encoded when read, so this kind of encoding will not appear in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "X8lVwKFt8_x1"
      },
      "source": [
        "### Problem 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "8b8cc31f-2ab4-4066-ae6b-998d72671b96",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "IZyCTUS-8_x2"
      },
      "source": [
        "®[2]`inp_lang` and `targ_lang` are our tokenizers, they both map \"`<start>`, `<end>`\" to \"1, 2\". How did this happen?  \n",
        "Hint: read src of the `fit_on_texts` function [here](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py#L199-L251).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh81_d-51BKT"
      },
      "source": [
        "\"fit_on_texts\" actually sort the words by their existing times. Both \"start\", and \"end\" exist in every sequence so they must be the top 2 in the sorted list. \"fit_on_texts\" also takes 0 our for padding so that 1 will be assigned to start and 2 will be assigned to \"end\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "pEtIlHpG8_x2"
      },
      "source": [
        "### Problem 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "8b655bb5-86de-400c-99f2-cc33924bfeef",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "umo-Hw4f8_x3"
      },
      "source": [
        "®[3] When setting up the vocabulary sizes, why do we need to add one on the length of `word_index` dict? Shouldn't the `word_index` already cover all tokens?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DYzlZ1O5-6B"
      },
      "source": [
        "We need to include \"padding\" in our vocabulary, which adds one more index to our dict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "HqJNaw8K8_x3"
      },
      "source": [
        "### Problem 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e60dc524-eae6-4d00-90de-8c216714d871",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "nHJtw9Lv8_x3"
      },
      "source": [
        "®[4] In the decoder framework, what do we set the dimensionality of our embedding matrix to be? Explain what each dimension represents and the reasoning behind we set the embedding matrix to be this way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZUh1nqZ6UXE"
      },
      "source": [
        "We set the embedding layer to be vocab_size on input and 256 on output. The output 256 is to cut down on the input dimensions to the rest of layers. That means our model will capture the inner strucutre of our vocabulary in a vector of length 256. The input is set to vocab_size since we need to let the embedding layer to know how many unique one-hot encoding words we have in our training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "NcLMhDHy8_x4"
      },
      "source": [
        "### Problem 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "d2c1da35-ad19-4bf8-9ae4-60cdec24a8c2",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "BY7IePhA8_x4"
      },
      "source": [
        "®[3]\n",
        "Which requires more parameters: single or multi-headed attention? Explain. Does this mean one necessarily trains faster than another?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9eUhWg0-o9V"
      },
      "source": [
        "Multi-headed attention requires more parameters. No, because each head in multi-head attention layer does not depend on each other, different heads can process attention in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "JBuSfGjn8_x4"
      },
      "source": [
        "### Problem 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "97a7c8a1-7a88-41a7-9b32-290ecc0731e6",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "FAH4-YdI8_x5"
      },
      "source": [
        "®[3]\n",
        "This model uses a particular attention mechanism. What is the name of this attention mechanism and what is the output shape?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFLuAfu2_CcX"
      },
      "source": [
        "The name is Bahdanau Attention. The output shape is (64, 16, 1) for (batch_size, seq_length, weight)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "GpNvDy_b8_x5"
      },
      "source": [
        "### Problem 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "24220773-bd3e-4e56-b7c5-de6a22e75c0c",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "b7OMeaXl8_x5"
      },
      "source": [
        "®[3] When instantiating the GRU layer we set the `return_sequences = True`. What does this argument control and why might we want this to be True?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0nSfqv-_e2m"
      },
      "source": [
        "We set \"return_sequences\" to True to get the full sequence output instead of only the last output. Since we are doing the translation instead of prediction, we need the full sequence output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "xvz8GD8q8_x6"
      },
      "source": [
        "### Problem 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "07fd6140-3679-48ff-90ec-a5ee1c6d124d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "bM3Ma7ca8_x6"
      },
      "source": [
        "®[3] What does the `mask` do in the loss function below, and why?\n",
        "```python\n",
        "\n",
        "def loss_function(real, pred):\n",
        " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        " loss_ = loss_object(real, pred)\n",
        "\n",
        " mask = tf.cast(mask, dtype=loss_.dtype)\n",
        " loss_ *= mask\n",
        "\n",
        " return tf.reduce_mean(loss_)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "prismiaParentId": "07fd6140-3679-48ff-90ec-a5ee1c6d124d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "UAXW7Heg8_x6"
      },
      "source": [
        "def loss_function(real, pred):\n",
        " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        " loss_ = loss_object(real, pred)\n",
        "\n",
        " mask = tf.cast(mask, dtype=loss_.dtype)\n",
        " loss_ *= mask\n",
        "\n",
        " return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntvZpxgrBmgP",
        "outputId": "6ebd2da6-a817-48c7-b999-8d30cfb5ff91"
      },
      "source": [
        "real = [1,2,3,0,0]\n",
        "pred = [1,2,3,0,0]\n",
        "tf.math.logical_not(tf.math.equal(real, 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=bool, numpy=array([ True,  True,  True, False, False])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXL6kI6NBfIR"
      },
      "source": [
        "The \"mask\" reverse the output of tf.math.equal. tf.math.equal compare the \"real\" to 0 to find out if there is any padding in the target vector. For loss function, we only calculate those which are not paddings. Thus, the mask set those positions which are padding to false. By multiplying loss_ with mask, we get rid of the calculation of paddings in our target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "Dq8Jvqis8_x7"
      },
      "source": [
        "### Problem 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "7777dd2f-1b9a-4ba1-b8ad-1d4cf9ccabbc",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "kxfsASzx8_x7"
      },
      "source": [
        "®[4] How does the tutorial set the RNN encoder's initial hidden state? Find all the lines of code that initialize the RNN encoder's hidden state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2buIeydgC65X"
      },
      "source": [
        "The tutorial set all the initial hidden state to zeros by tf.zeros function. The codes are here:\n",
        "\n",
        "def initialize_hidden_state(self):\\\n",
        "return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "def call(self, x, hidden):\\\n",
        "    x = self.embedding(x) \\\n",
        "    output, state = self.gru(x, initial_state=hidden) \\\n",
        "    return output, state \\\n",
        "\n",
        "Here, the input \"hidden\" is set to the output of the \"initialize_hidden_state\" function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "Xda0o28R8_x8"
      },
      "source": [
        "### Problem 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "bc7cf3bb-6ab0-4b4a-8b8a-f6d964f05fa8",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "RwsHeEde8_x8"
      },
      "source": [
        "®[4] How does the tutorial set the RNN decoder's initial hidden state? Find all the lines of code that initialize the RNN decoder's hidden state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-FI5MzaD4hp"
      },
      "source": [
        "The tutorial set the RNN decoder's initial hidden state to encoder's output hidden state. The codes are here:\n",
        "\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "nHw3QyRt8_x9"
      },
      "source": [
        "### Problem 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "9a7c967c-ff84-4dbe-b672-2744e04f85bb",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "KxdSZcod8_x9"
      },
      "source": [
        "®[5] What's the search algorithm in the `evaluate(sentence)` function? Why could it miss the optimal sequence? Why is the beam search a better solution?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHmkPeNgFv2b"
      },
      "source": [
        "The search algorithm in evaluate(sentence) only pick the word with the highest probability in the predictions in each iteration before it reaches max sequence length. However, the sequence is combination, if we pick words with the highest probability at each postion, the whole sequence might not be the optimal sequence since the combination is not good between words. For example, the first output in evaluate is A and the second one is B. However, the optimal output should be C and B, but C's probability is lower than A and we take it out. Beam search is better beacuse, with a beam width, we will pick several top predictions in current iteration, and pick the next based on the optimality of the combination of the next and the current sequence. In this way, we are less likely to miss the optimal sequence like \"C\" \"B\" in the above scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "XtViithN8_x9"
      },
      "source": [
        "### Problem 17"
      ]
    }
  ]
}