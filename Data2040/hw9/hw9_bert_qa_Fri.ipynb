{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw9-bert-qa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5446247bfc84353994bebff27e938dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_57cef1ddb039485192e107dcac2a96d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10fd8fe69f0b4dfcbda5f29d23a45421",
              "IPY_MODEL_291de0d6a38e4fd8a96af145aac5c9cd"
            ]
          }
        },
        "57cef1ddb039485192e107dcac2a96d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10fd8fe69f0b4dfcbda5f29d23a45421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_72399ff96efc4085a752d8742483660e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9256bef27114f0d9d6eb652b2a15b6e"
          }
        },
        "291de0d6a38e4fd8a96af145aac5c9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9a128c9a1684200b5e88b894e157d8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.30kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9d80dc9ec8e4ebba6828d19e6298c7f"
          }
        },
        "72399ff96efc4085a752d8742483660e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9256bef27114f0d9d6eb652b2a15b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9a128c9a1684200b5e88b894e157d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9d80dc9ec8e4ebba6828d19e6298c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b794e9f9642345819e823bce89428b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_40603d1b67b449e7a6e6c7ec6c23b57f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_90a8266975664c5b80b999c293223c19",
              "IPY_MODEL_f187cc7c9ef640cf9cfb537516561a84"
            ]
          }
        },
        "40603d1b67b449e7a6e6c7ec6c23b57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90a8266975664c5b80b999c293223c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e79dce824fc741168c1c10a05af7eb84",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0b42ecf7e3444f5b1d632a6e4f9dcdf"
          }
        },
        "f187cc7c9ef640cf9cfb537516561a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a521b6d10ba4629a760135481d58bdc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:09&lt;00:00, 55.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63ee9c89362141db8e24d95f5649cb03"
          }
        },
        "e79dce824fc741168c1c10a05af7eb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0b42ecf7e3444f5b1d632a6e4f9dcdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a521b6d10ba4629a760135481d58bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63ee9c89362141db8e24d95f5649cb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "431be66109a14519b54689c243e5eb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dffe840e2d97413f938863cbff80cf40",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f18aef1b4c84b40bb2a9e4c46bd5860",
              "IPY_MODEL_1cf1df4fddb448aeb69436e46d7b0583"
            ]
          }
        },
        "dffe840e2d97413f938863cbff80cf40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f18aef1b4c84b40bb2a9e4c46bd5860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_922bc5a0760a4a848fe5d4661c0a6617",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0412b1976c51430a83e54366f5cf8fcf"
          }
        },
        "1cf1df4fddb448aeb69436e46d7b0583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0575d60b516640778b3b918f80f1a755",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 568kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ebb2b361c32248f59d8a9b3a283f215e"
          }
        },
        "922bc5a0760a4a848fe5d4661c0a6617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0412b1976c51430a83e54366f5cf8fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0575d60b516640778b3b918f80f1a755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ebb2b361c32248f59d8a9b3a283f215e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f67e99e666da49749576e8a4a2f4ba2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a869bd292f4b41c1829daaea035201ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e904cf3f7b242d491d03e40583c9b2b",
              "IPY_MODEL_26560adb2cc04cdc84b4e43ac3c89fc8"
            ]
          }
        },
        "a869bd292f4b41c1829daaea035201ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e904cf3f7b242d491d03e40583c9b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e6fac1d4fd4417d81d3dae93783b1d3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad404441b2f94a1b8d1756dcb50fad50"
          }
        },
        "26560adb2cc04cdc84b4e43ac3c89fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_23af5ba0641d43c7921a63a23fdc661a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 107B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25aa7730aaf74a67801f79a6aaa2feb6"
          }
        },
        "4e6fac1d4fd4417d81d3dae93783b1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad404441b2f94a1b8d1756dcb50fad50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23af5ba0641d43c7921a63a23fdc661a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25aa7730aaf74a67801f79a6aaa2feb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd813afbe92644bda0c7abfb0d401085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a86d399c06c7437187c5cdb3c80f2c20",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_afa2d703ec0d4559858a3e549bdf5a4c",
              "IPY_MODEL_50cc2e3cf0334c78bd5e16ce0f9a93b3"
            ]
          }
        },
        "a86d399c06c7437187c5cdb3c80f2c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afa2d703ec0d4559858a3e549bdf5a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_daef5a3eecda48969a93f1392d18e5fe",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d50ceb0482db4351bfd6491e28b5beee"
          }
        },
        "50cc2e3cf0334c78bd5e16ce0f9a93b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6278d154b02343819b546ae41628dcfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 5.52MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_023856f10d54435c9b8ae42751f69b8e"
          }
        },
        "daef5a3eecda48969a93f1392d18e5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d50ceb0482db4351bfd6491e28b5beee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6278d154b02343819b546ae41628dcfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "023856f10d54435c9b8ae42751f69b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "jLqs7QZN90_4"
      },
      "source": [
        "### Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "2f08f224-eab0-43b3-b5e6-7961b7b75a4f",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "nmI9vC5L90_-"
      },
      "source": [
        "**Note:** This assignment is due on Sunday, April 25th at Noon ET and will be graded Pass/Fail.  No extensions allowed.\n",
        "\n",
        "Dear Students,\n",
        "\n",
        "Welcome to the last homework in DATA2040-sp21.  You can work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n",
        " \n",
        "\n",
        "The exercises in this homework are designed to help ensure you understand the setup for pre-training a BERT model and how to fine-tune BERT models for specific applications.\n",
        " \n",
        "After completing this assignment, you’ll understand the connection between BERT models and BERT applications and have the know-how to perform fine-tuning for Extractive Question and Answering (EQA) using a Keras head model (running on GPUs and TPUs) and with the standard HuggingFace.co transformer library (running on GPUs).\n",
        " \n",
        "Along the way, you’ll demonstrate your understanding of the data preprocessing methods required for both BERT and EQA and your knowledge of applicable training loss functions and metrics.\n",
        " \n",
        "**Primary Resource**\n",
        "This paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) introduced BERT to the world. Please use it as your primary reference while working on this assignment, i.e., try and work through the relevant parts. \n",
        "\n",
        "Why? Because papers like this one are the format in which most new Deep Learning techniques are introduced. It often takes several years (and sometimes decades) before secondary materials become available for the methods that become recognized as important.\n",
        "\n",
        "**Additional Resources **\n",
        "Once you've taken a shot at it, please feel free to access other materials and google.  Below is a curated list of suggestions.\n",
        "\n",
        "First up is the paper's official github [repo](https://github.com/google-research/bert).  Mapping papers to code, and code to papers is a great way to improve your practical DL skills. The code is well organized, but complex; here is one file that may be particularly helpful to took at  [run_pretraining.py](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L131)\n",
        "\n",
        "Next up areJay Alammar's blog posts on Transformers and BERT:\n",
        "\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
        "\n",
        "\n",
        "**The Future of DL (and Data Science)**\n",
        "Up to now, understanding what's been learned by complex NLP models has been a challenge.  Alammar's Pyhton [Ecco](https://www.eccox.io/) package is one attempt to change that. It provides visualizations of the information in Transformer models. These two blog posts provide an introduction to the work so far:\n",
        "\n",
        "- [Interfaces for Explaining Transformer Language Models](http://jalammar.github.io/explaining-transformers/)\n",
        "- [Find the Words to Say: Hidden State Visualization for Language Models](http://jalammar.github.io/hidden-states/)\n",
        "\n",
        "\n",
        "People that have a deep understanding of complex structures are in a good position to create innovative applications based on them and to perhaps develop new and important structures based on this knowledge.  The development of BERT is a perfect example.  \n",
        "\n",
        "\n",
        "Good luck wrapping up your Final Projects and this assignment.\n",
        " \n",
        "On behalf of the entire course staff, I would like to thank you for participating in DATA 2040. It has been a pleasure to have you all as students.\n",
        "\n",
        "Best Wishes,\n",
        "Dan Potter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "ae320a95-f6ad-48f4-a7c0-661d05c4beef",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "fvot9ono91AA"
      },
      "source": [
        "**Partner Option**\n",
        "You may work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n",
        "**®Task: **If you worked with a partner on this assignment, please list their official email address below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ETuD_Pe991AB"
      },
      "source": [
        "### Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "f47e8206-12ff-4e1e-83b9-40eb15b997ee",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "qLC0Yd1D91AB"
      },
      "source": [
        "**Partner Option**\n",
        "**®Task:** If you worked with a partner, please confirm that you and your partner both understand that only one of you may submit this homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "LCJl8ng-91AB"
      },
      "source": [
        "### Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "76d1fb4e-3ee7-460f-95cb-0169d64aae47",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "jRJMuR9u91AB"
      },
      "source": [
        "**Pretraining BERT**\n",
        "\n",
        "**®[15] Task: A is for Attention, B is for Bert, C is for Cross-Entropy Loss **\n",
        "Describe how BERT is pretrained.  \n",
        "1. Include an explanation of the what training inputs look like, including the labels that are learned.\n",
        "2. As part of this, include a few specific training input examples.  Include details on any special tokens used (like [CLS], [SEP], [MASK]), and what the corresponding labels to learn are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPzfr0zjK8KV"
      },
      "source": [
        "In general, the BERT is trained using two unsupervised task, Masked Language Modeling and Next Sentence Prediction. The loss used here is Cross Entropy to between the labels and predictions. \n",
        "\n",
        "1. To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\n",
        "The training input data includes BooksCorpus and English WikiPedia text messages. The start of the sentence is with [CLS], masked tokens with [MASK] and seperate between sequences with [SEP]. Also, the padding is used to have fixed length.\n",
        "\n",
        "2. 15% of the word tokens in the sequence will be masked and 80% of the time, the token is masked with special token [MASK], 10% of the time unchanged and 10% of the time with random token to force BERT to learn from the previous and later tokens instead of learning from the word itself. The training use Cross-Entropy Loss.\n",
        "Also for NSP, 50% of the time, the sentences are in order and 50% percent of the time the second sentence is actually not the next one.\n",
        "\n",
        "Masked LM: \\\\\n",
        "The input: [CLS] the man went to [MASK] store [SEP] he bought a bottle of [MASK] coke [SEP] \\\\\n",
        "The output: the diet \\\\\n",
        "The input: [CLS] I love reading [MASK] \\\\\n",
        "The output: books \n",
        "\n",
        "NSP:\\\\\n",
        "The input: [CLS] I am a Brown [MASK] [SEP] I study [MASK] science [SEP] \\\\\n",
        "The output: IsNext \\\\\n",
        "The input: [CLS] The day is [MASK] [SEP] NBA pauses due to [Mask] [SEP] \\\\\n",
        "The output: NotNext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "YZozvkfF91AC"
      },
      "source": [
        "### Problem 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a0433bcf-fdac-4ab7-9c6f-e623cb90530d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "y2KbjmH391AC"
      },
      "source": [
        "**Pretraining BERT**\n",
        "\n",
        "**®[5] Task: Attention to detailZzzz**\n",
        "How is the loss function defined?  Since BERT is trained on more than one task, how are they combined?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgkOseIVYhBX"
      },
      "source": [
        "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "B8ZeTgor91AD"
      },
      "source": [
        "### Problem 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "b510a209-203a-4b5c-b0d8-20f06c6639eb",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "cIudSQ7t91AD"
      },
      "source": [
        "**Using pre-trained BERT models**\n",
        "\n",
        "**®[10] Task: Machine Intelligence?**\n",
        "The BERT paper explains how to configure and finetune a pre-trained BERT model to perform various NLP tasks. \n",
        "1. What are the specific tasks described? \n",
        "2. At which tasks was BERT documented as being state of the art?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrKnvXYKY2L6"
      },
      "source": [
        "1. GLUE (General Language Understanding Evaluation), SQuAD v1.1 (Stanford Question Answering Dataset),  SQuAD v2.0 and SWAG (Situations With Adversarial Generations).\n",
        "\n",
        "2. SQuAD v2.0, MNLI (Multi-Genre Natural Language Inference), QQP (Quora Question Pairs), QNLI (Question Natural Language Inference ), SST-2 (The Stanford Sentiment Treebank), CoLA (The Corpus of Linguistic Acceptability), STS-B (The Semantic Textual Similarity Benchmark), MRPC (Microsoft Research Paraphrase Corpus), RTE (Recognizing Textual Entailment), WNLI (Winograd NLI), SWAG (Situations With Adversarial Generations). In total, BERT advanced 11 NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "h2dmodyb91AD"
      },
      "source": [
        "### Problem 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e7f61f66-dbaf-4fa2-bcfd-78fce60e2630",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "llkc25Tj91AE"
      },
      "source": [
        "**Using pre-trained BERT models**\n",
        "\n",
        "**®[5] Task: School versus learning a new task**\n",
        "Which takes longer, pre-training or fine-tuning? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyeFIrBXdYMn"
      },
      "source": [
        "Pre-training. Since fine-tuning of BERT is sensitive to small datasets than large-scaled datasets, and only a few parameters will be fine-tuned during the process. Also, hyperparameters are chosen in a given range with most of them same as those in pre-training. Furthermore, fine-tuning only cares about a specific task and uses features generated from pre-training to solve speicfic problems. Therefore, fine-tuning is faster than pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "_j-xplsE91AE"
      },
      "source": [
        "### Problem 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e5061573-2707-4d57-a78b-ff48d9e1e852",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "XtLB-w1W91AE"
      },
      "source": [
        "**Extractive question and answering**\n",
        "Please review the following extractive question and answer (EQA) problems before beginning to answer them.  Each one will be graded on the information requested. \n",
        "\n",
        "**®[15] Task: Focus on the answer**\n",
        "1. What is extractive question and answering? \n",
        "2. What is [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)? \n",
        "3. Provide an example of the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPriheiveS5V"
      },
      "source": [
        "1. Given a question and passage, the model should predict the answer or provide the answer text to that question.\n",
        "\n",
        "2. SQuAD1.1 is a collection of 100k crowdsourced question/answer pairs. The answer in the label is directly cutted from a part of the passages. Three ground truth answers will usually contain three different labels (same if no enough).\n",
        "\n",
        "3. Question: What is a course of study called? \\\\\n",
        "Passage: The role of teacher is often formal and ongoing, carried out at a school or other place of formal education. In many countries, a person who wishes to become a teacher must first obtain specified professional qualifications or credentials from a university or college. These professional qualifications may include the study of pedagogy, the science of teaching. Teachers, like other professionals, may have to continue their education after they qualify, a process known as continuing professional development. Teachers may use a lesson plan to facilitate student learning, providing a course of study which is called the curriculum.\\\\\n",
        "Ground Truth Answers: the curriculum. curriculum curriculum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "bdihoyNO91AE"
      },
      "source": [
        "### Problem 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "8d3b86f3-ae46-4125-bf1c-2fbacb3959d8",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "rSf1lFTu91AE"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®Task [10]: Teaching BERT to do EQA #1**\n",
        "Describe the BERT fine-tuning process for extractive question and answering.  Assume the SQuAD1.1 dataset is being used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxEKnzNKhKGu"
      },
      "source": [
        "The question and paragraph are combined in to a single pack as input into BERT with different embeddings for their identities. S and E for start and end are introduced for the output to limit a span of text from passage. The probability of word i being the\n",
        "start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph. The training objective is the sum of the log-likelihoods of the correct start and end positions. The developers fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. They also use data augmentation by fine-tuning TriviaQA before fine-tuning with SQuAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "mVofddtW91AF"
      },
      "source": [
        "### Problem 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a0590c33-dc37-4390-9d4c-c6cfec034619",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "9ryJNwG891AF"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[10] Task: Teaching BERT to do EQA #2**\n",
        "Provide two training input examples, and details on [CLS], [SEP], [MASK], and labels. It would be enough if the format is correct. \n",
        "(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ParblrbnvVTB"
      },
      "source": [
        "1. [CLS] who is dog [sep] alice needs to walk dog tom every afternoon [sep]\n",
        "\n",
        "2. [CLS] who studies at brown [sep] alice visits tom at brown in the last summer [sep]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "tZpLNais91AF"
      },
      "source": [
        "### Problem 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a7a44d60-ccb5-47af-b4be-7069d32ad1bc",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "7oI7JdY291AF"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[10] Task: Teaching BERT to do EQA #3**\n",
        "Confirm your training sample input understanding by running them through the the preprocessing code in this [notebook](https://keras.io/examples/nlp/text_extraction_with_bert/) (keras.io). \n",
        "\n",
        "Copy and paste the output of your computer experiment below and explain it. (Hint: you may find that the paper and the code change the order of some things, that is OK)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Kwkv3_wr_S"
      },
      "source": [
        "This is the code I used to go through the process and check output:\\\\\n",
        "\n",
        "```\n",
        "squad_eg = SquadExample('who is dog', 'alice needs to walk dog tom every afternoon', 24, ['tom'], [['tom']])\n",
        "squad_eg.preprocess()\n",
        "tokenizer.decode(squad_eg.input_ids)\n",
        "squad_eg.input_ids\n",
        "```\n",
        "The decoded output: alice needs to walk dog tom every afternoon who is dog\\\\\n",
        "The encoded output: [101,5650,3791,2000,3328,3899,3419,2296,5027,102,2040,2003,\n",
        "3899,102,0,0,0,0,0...] \\\\\n",
        "We can see that the 101 means start [CLS] and 102 is the token for [SEP]. The question and context are merged into one input.\n",
        "\n",
        "\n",
        "```\n",
        "squad_eg = SquadExample('who studies at brown', 'alice visits tom at brown in the last summer', 13, ['tom'], [['tom']])\n",
        "squad_eg.preprocess()\n",
        "squad_eg.input_ids\n",
        "```\n",
        "The decoded output: alice visits tom at brown in the last summerwho studies at brown \\\\\n",
        "The encoded output: [101,5650,7879,3419,2012,2829,1999,1996,2197,2621,102,2040,2913,2012,2829,102,0,0,0,0...] \\\\\n",
        "We can see that the 101 means start [CLS] and 102 is the token for [SEP]. The question and context are merged into one input.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "7e1kiFJS91AG"
      },
      "source": [
        "### Problem 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e6ba1d0e-ff4b-47a2-ae40-33aae7cfa207",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "nDipJy7h91AG"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[5] Task: Teaching BERT to do EQA #4**\n",
        "The pre-trained BERT model has multiple outputs; which ones are used for fine-tuning the squad task and which ones are ignored?\n",
        "(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKG02qJ0xqkF"
      },
      "source": [
        "The probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph. Take the state of last hidden layer and feed it into the start token classifier. The start token classifier only has a single set of weights which it applies to every word. After taking the dot product between the output embeddings and the start weights, it applies the softmax activation to produce a probability distribution over all of the words. Whichever word has the highest probability of being the start token is the one that we pick. It also repeats this process for the end token — we have a separate weight vector for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "kpCPaf0291AG"
      },
      "source": [
        "### Problem 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "4545897b-b70d-4b85-9ffc-fd62cdc0c6d8",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "Y0DMGv0k91AG"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®[5] Task: Teaching BERT to do EQA #5**\n",
        "Identify and describe the loss function that is used. Provide sufficient detail for the reader to understand why it’s appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r68myZ-VDBux"
      },
      "source": [
        "Code for loss in github:\n",
        "```\n",
        " def compute_loss(logits, positions):\n",
        "        one_hot_positions = tf.one_hot(\n",
        "            positions, depth=seq_length, dtype=tf.float32)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        loss = -tf.reduce_mean(\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "        return loss\n",
        "```\n",
        "\n",
        "\n",
        "The training objective is the sum of the log-likelihoods of the correct start and end positions. Since the answer in SQuAD is a text span that comes from the paragraph, we only need to specify the start and end character index in the paragraph to describe the label and output. Under this schema, the loss is the average of the start_position loss start_loss and the end_postition loss end_loss. Each loss is computed in the same way: after applying the Softmax to the final output (i.e S*Ti or E*Ti) we use the real start/end postitions to compute the loss (see code below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "9bFV2cA591AH"
      },
      "source": [
        "### Problem 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "206eeb8f-9039-4938-9a50-3ff8b707acb9",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "pLtEN0I991AH"
      },
      "source": [
        "**Extractive question and answering**\n",
        "The keras.io notebook includes code to take advantage of TPU’s. It requires the model weights to be represented explicitly in the tensorflow model graph. \n",
        "\n",
        "**®[5] Task: Run with the wind #1**\n",
        "Copy and paste the code cell(s) that expose the head model and compile it within a TPU context.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVEphe5SF3Qx",
        "outputId": "b6571220-cee6-4338-e042-b22809406ab0"
      },
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 17.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.2\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=8f810937d7f31b7b1cb56ab86cc77e4a5fab922a281a1a18fa3f480f0ac03292\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d5446247bfc84353994bebff27e938dd",
            "57cef1ddb039485192e107dcac2a96d8",
            "10fd8fe69f0b4dfcbda5f29d23a45421",
            "291de0d6a38e4fd8a96af145aac5c9cd",
            "72399ff96efc4085a752d8742483660e",
            "a9256bef27114f0d9d6eb652b2a15b6e",
            "c9a128c9a1684200b5e88b894e157d8f",
            "d9d80dc9ec8e4ebba6828d19e6298c7f",
            "b794e9f9642345819e823bce89428b4e",
            "40603d1b67b449e7a6e6c7ec6c23b57f",
            "90a8266975664c5b80b999c293223c19",
            "f187cc7c9ef640cf9cfb537516561a84",
            "e79dce824fc741168c1c10a05af7eb84",
            "f0b42ecf7e3444f5b1d632a6e4f9dcdf",
            "7a521b6d10ba4629a760135481d58bdc",
            "63ee9c89362141db8e24d95f5649cb03"
          ]
        },
        "id": "7acNNqwlFT1Y",
        "outputId": "6f747ef3-8b89-48ef-fadf-7a8a13b3440c"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "max_len = 384\n",
        "configuration = BertConfig()\n",
        "\n",
        "def create_model():\n",
        "    ## BERT encoder\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    ## QA Model\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    )[0]\n",
        "\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n",
        "\n",
        "use_tpu = False # change this to use tpu or gpu\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5446247bfc84353994bebff27e938dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b794e9f9642345819e823bce89428b4e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8cf621bd70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8cf621bd70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f8d11ac8c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f8d11ac8c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       768         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       768         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,776\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "crFHiA0N91AH"
      },
      "source": [
        "### Problem 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "5b37fcca-6b19-4d45-84ee-50073227c494",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "lYv49ROP91AH"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®[5] Task: Run with the wind #2 **\n",
        "Using a batch size of 16, try fine-tuning with TPU vs. GPU enabled. Report the speed change below. How much GPU memory is used with a batch size of 16?  \n",
        "\n",
        "Hints:\n",
        "1. If you setup wandb.ai before you begin your experiments, you can use the System panel  ![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-76da31d9-93b6-49ae-a0ca-356e26e85e8f.png?alt=media&token=205261cb-6984-4246-b866-938082d192c1) to display system resource usage over time.\n",
        "2. GPUs are slower than TPUs for this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268,
          "referenced_widgets": [
            "431be66109a14519b54689c243e5eb8f",
            "dffe840e2d97413f938863cbff80cf40",
            "8f18aef1b4c84b40bb2a9e4c46bd5860",
            "1cf1df4fddb448aeb69436e46d7b0583",
            "922bc5a0760a4a848fe5d4661c0a6617",
            "0412b1976c51430a83e54366f5cf8fcf",
            "0575d60b516640778b3b918f80f1a755",
            "ebb2b361c32248f59d8a9b3a283f215e",
            "f67e99e666da49749576e8a4a2f4ba2d",
            "a869bd292f4b41c1829daaea035201ff",
            "4e904cf3f7b242d491d03e40583c9b2b",
            "26560adb2cc04cdc84b4e43ac3c89fc8",
            "4e6fac1d4fd4417d81d3dae93783b1d3",
            "ad404441b2f94a1b8d1756dcb50fad50",
            "23af5ba0641d43c7921a63a23fdc661a",
            "25aa7730aaf74a67801f79a6aaa2feb6",
            "dd813afbe92644bda0c7abfb0d401085",
            "a86d399c06c7437187c5cdb3c80f2c20",
            "afa2d703ec0d4559858a3e549bdf5a4c",
            "50cc2e3cf0334c78bd5e16ce0f9a93b3",
            "daef5a3eecda48969a93f1392d18e5fe",
            "d50ceb0482db4351bfd6491e28b5beee",
            "6278d154b02343819b546ae41628dcfb",
            "023856f10d54435c9b8ae42751f69b8e"
          ]
        },
        "id": "gFamy8f-HI0x",
        "outputId": "c7c34ab3-a82c-49aa-fd94-221eebc2d4dc"
      },
      "source": [
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n",
        "\n",
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)\n",
        "\n",
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Clean context, answer and question\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        # Find end character index of answer in context\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in context that are in answer\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # Find tokens that were created from answer characters\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
        "            tokenized_question.ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "431be66109a14519b54689c243e5eb8f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f67e99e666da49749576e8a4a2f4ba2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd813afbe92644bda0c7abfb0d401085",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "4857856/4854279 [==============================] - 0s 0us/step\n",
            "87599 training points created.\n",
            "10570 evaluation points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnC9g7FaHTuI"
      },
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `SquadExample` object contains the character level offsets for each token\n",
        "    in its input paragraph. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `SquadExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "mdcQq4L8Gg9g",
        "outputId": "52e90286-b109-4372-a625-19c9edca9a3e"
      },
      "source": [
        "import time\n",
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "start = time.time()\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=2,\n",
        "    batch_size=16,\n",
        "    callbacks=[exact_match_callback],\n",
        ")\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-30ec955b3967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexact_match_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty3ZhpMZhswg",
        "outputId": "2d6dda88-fac3-4d0f-8e48-6f8932cda374"
      },
      "source": [
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6093.306775808334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOM_u853HomA"
      },
      "source": [
        "Time with tpu: 518.122209072113 seconds \\\\\n",
        "Time with gpu: I did not finish running within 1 hour. So I just print out the end time right after I interrupt the running and calculate the time elapsed: 6093.306775808334 seconds \\\\\n",
        "It uses 6.06 GB memory and 14 GB GPU disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ZbKOcmrm91AH"
      },
      "source": [
        "### Problem 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "628736ea-ead0-4967-bba8-d7357ded5147",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "c9wKblgH91AI"
      },
      "source": [
        "**HuggingFace in practice**\n",
        "\n",
        "Hugging face[ pipelines](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) are an effortless way to use pre-trained and pre-finetuned models. As we saw in the last homework, Hugging Face also provides tools for fine-tuning.\n",
        "\n",
        "**®[5] Task:  It's time to Hang 10**\n",
        "Run the** **question and answer fine-tuning notebook from [The Big Table of Tasks](https://huggingface.co/transformers/examples.html?highlight=big%20table%20tasks#the-big-table-of-tasks) and run it. In the cell below, provide the final exact match and f-1 scores and explain what they mean.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ahQ3FTKgb0"
      },
      "source": [
        "{'exact_match': 76.74550614947965, 'f1': 85.13412652023338}\n",
        "\n",
        "These scores are computed on individual question+answer pairs. When multiple correct answers are possible for a given question, the maximum score over all possible correct answers is computed. Overall EM and F1 scores are computed for a model by averaging over the individual example scores.\n",
        "\n",
        "**Exact Match:** This metric is as simple as it sounds. For each question+answer pair, if the characters of the model's prediction exactly match the characters of (one of) the True Answer(s), EM = 1, otherwise EM = 0.\n",
        "\n",
        "**F1 Score:** In this case, it's computed over the individual words in the prediction against those in the True Answer. The number of shared words between the prediction and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the prediction, and recall is the ratio of the number of shared words to the total number of words in the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ex0Op6Je91AI"
      },
      "source": [
        "### Problem 16"
      ]
    }
  ]
}