{"metadata":{},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 1"]},{"cell_type":"markdown","metadata":{"prismiaId":"2f08f224-eab0-43b3-b5e6-7961b7b75a4f","trusted":true,"editable":false,"deletable":false},"source":["**Note:** This assignment is due on Sunday, April 25th at Noon ET and will be graded Pass/Fail.  No extensions allowed.\n\nDear Students,\n\nWelcome to the last homework in DATA2040-sp21.  You can work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n \n\nThe exercises in this homework are designed to help ensure you understand the setup for pre-training a BERT model and how to fine-tune BERT models for specific applications.\n \nAfter completing this assignment, you’ll understand the connection between BERT models and BERT applications and have the know-how to perform fine-tuning for Extractive Question and Answering (EQA) using a Keras head model (running on GPUs and TPUs) and with the standard HuggingFace.co transformer library (running on GPUs).\n \nAlong the way, you’ll demonstrate your understanding of the data preprocessing methods required for both BERT and EQA and your knowledge of applicable training loss functions and metrics.\n \n**Primary Resource**\nThis paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) introduced BERT to the world. Please use it as your primary reference while working on this assignment, i.e., try and work through the relevant parts. \n\nWhy? Because papers like this one are the format in which most new Deep Learning techniques are introduced. It often takes several years (and sometimes decades) before secondary materials become available for the methods that become recognized as important.\n\n**Additional Resources **\nOnce you've taken a shot at it, please feel free to access other materials and google.  Below is a curated list of suggestions.\n\nFirst up is the paper's official github [repo](https://github.com/google-research/bert).  Mapping papers to code, and code to papers is a great way to improve your practical DL skills. The code is well organized, but complex; here is one file that may be particularly helpful to took at  [run_pretraining.py](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L131)\n\nNext up areJay Alammar's blog posts on Transformers and BERT:\n\n- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n\n\n**The Future of DL (and Data Science)**\nUp to now, understanding what's been learned by complex NLP models has been a challenge.  Alammar's Pyhton [Ecco](https://www.eccox.io/) package is one attempt to change that. It provides visualizations of the information in Transformer models. These two blog posts provide an introduction to the work so far:\n\n- [Interfaces for Explaining Transformer Language Models](http://jalammar.github.io/explaining-transformers/)\n- [Find the Words to Say: Hidden State Visualization for Language Models](http://jalammar.github.io/hidden-states/)\n\n\nPeople that have a deep understanding of complex structures are in a good position to create innovative applications based on them and to perhaps develop new and important structures based on this knowledge.  The development of BERT is a perfect example.  \n\n\nGood luck wrapping up your Final Projects and this assignment.\n \nOn behalf of the entire course staff, I would like to thank you for participating in DATA 2040. It has been a pleasure to have you all as students.\n\nBest Wishes,\nDan Potter\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"ae320a95-f6ad-48f4-a7c0-661d05c4beef","trusted":true,"editable":false,"deletable":false},"source":["**Partner Option**\nYou may work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n**®Task: **If you worked with a partner on this assignment, please list their official email address below.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 2"]},{"cell_type":"markdown","metadata":{"prismiaId":"f47e8206-12ff-4e1e-83b9-40eb15b997ee","trusted":true,"editable":false,"deletable":false},"source":["**Partner Option**\n**®Task:** If you worked with a partner, please confirm that you and your partner both understand that only one of you may submit this homework.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 3"]},{"cell_type":"markdown","metadata":{"prismiaId":"76d1fb4e-3ee7-460f-95cb-0169d64aae47","trusted":true,"editable":false,"deletable":false},"source":["**Pretraining BERT**\n\n**®[15] Task: A is for Attention, B is for Bert, C is for Cross-Entropy Loss **\nDescribe how BERT is pretrained.  \n1. Include an explanation of the what training inputs look like, including the labels that are learned.\n2. As part of this, include a few specific training input examples.  Include details on any special tokens used (like [CLS], [SEP], [MASK]), and what the corresponding labels to learn are.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 4"]},{"cell_type":"markdown","metadata":{"prismiaId":"a0433bcf-fdac-4ab7-9c6f-e623cb90530d","trusted":true,"editable":false,"deletable":false},"source":["**Pretraining BERT**\n\n**®[5] Task: Attention to detailZzzz**\nHow is the loss function defined?  Since BERT is trained on more than one task, how are they combined?\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 5"]},{"cell_type":"markdown","metadata":{"prismiaId":"b510a209-203a-4b5c-b0d8-20f06c6639eb","trusted":true,"editable":false,"deletable":false},"source":["**Using pre-trained BERT models**\n\n**®[10] Task: Machine Intelligence?**\nThe BERT paper explains how to configure and finetune a pre-trained BERT model to perform various NLP tasks. \n1. What are the specific tasks described? \n2. At which tasks was BERT documented as being state of the art?\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 6"]},{"cell_type":"markdown","metadata":{"prismiaId":"e7f61f66-dbaf-4fa2-bcfd-78fce60e2630","trusted":true,"editable":false,"deletable":false},"source":["**Using pre-trained BERT models**\n\n**®[5] Task: School versus learning a new task**\nWhich takes longer, pre-training or fine-tuning? Why?\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 7"]},{"cell_type":"markdown","metadata":{"prismiaId":"e5061573-2707-4d57-a78b-ff48d9e1e852","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\nPlease review the following extractive question and answer (EQA) problems before beginning to answer them.  Each one will be graded on the information requested. \n\n**®[15] Task: Focus on the answer**\n1. What is extractive question and answering? \n2. What is [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)? \n3. Provide an example of the task.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 8"]},{"cell_type":"markdown","metadata":{"prismiaId":"8d3b86f3-ae46-4125-bf1c-2fbacb3959d8","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n\n**®Task [10]: Teaching BERT to do EQA #1**\nDescribe the BERT fine-tuning process for extractive question and answering.  Assume the SQuAD1.1 dataset is being used.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 9"]},{"cell_type":"markdown","metadata":{"prismiaId":"a0590c33-dc37-4390-9d4c-c6cfec034619","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n**®[10] Task: Teaching BERT to do EQA #2**\nProvide two training input examples, and details on [CLS], [SEP], [MASK], and labels. It would be enough if the format is correct. \n(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 10"]},{"cell_type":"markdown","metadata":{"prismiaId":"a7a44d60-ccb5-47af-b4be-7069d32ad1bc","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n**®[10] Task: Teaching BERT to do EQA #3**\nConfirm your training sample input understanding by running them through the the preprocessing code in this [notebook](https://keras.io/examples/nlp/text_extraction_with_bert/) (keras.io). \n\nCopy and paste the output of your computer experiment below and explain it. (Hint: you may find that the paper and the code change the order of some things, that is OK)\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 11"]},{"cell_type":"markdown","metadata":{"prismiaId":"e6ba1d0e-ff4b-47a2-ae40-33aae7cfa207","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n**®[5] Task: Teaching BERT to do EQA #4**\nThe pre-trained BERT model has multiple outputs; which ones are used for fine-tuning the squad task and which ones are ignored?\n(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 12"]},{"cell_type":"markdown","metadata":{"prismiaId":"4545897b-b70d-4b85-9ffc-fd62cdc0c6d8","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n\n**®[5] Task: Teaching BERT to do EQA #5**\nIdentify and describe the loss function that is used. Provide sufficient detail for the reader to understand why it’s appropriate.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 13"]},{"cell_type":"markdown","metadata":{"prismiaId":"206eeb8f-9039-4938-9a50-3ff8b707acb9","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\nThe keras.io notebook includes code to take advantage of TPU’s. It requires the model weights to be represented explicitly in the tensorflow model graph. \n\n**®[5] Task: Run with the wind #1**\nCopy and paste the code cell(s) that expose the head model and compile it within a TPU context.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 14"]},{"cell_type":"markdown","metadata":{"prismiaId":"5b37fcca-6b19-4d45-84ee-50073227c494","trusted":true,"editable":false,"deletable":false},"source":["**Extractive question and answering**\n\n**®[5] Task: Run with the wind #2 **\nUsing a batch size of 16, try fine-tuning with TPU vs. GPU enabled. Report the speed change below. How much GPU memory is used with a batch size of 16?  \n\nHints:\n1. If you setup wandb.ai before you begin your experiments, you can use the System panel  ![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-76da31d9-93b6-49ae-a0ca-356e26e85e8f.png?alt=media&token=205261cb-6984-4246-b866-938082d192c1) to display system resource usage over time.\n2. GPUs are slower than TPUs for this task.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 15"]},{"cell_type":"markdown","metadata":{"prismiaId":"628736ea-ead0-4967-bba8-d7357ded5147","trusted":true,"editable":false,"deletable":false},"source":["**HuggingFace in practice**\n\nHugging face[ pipelines](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) are an effortless way to use pre-trained and pre-finetuned models. As we saw in the last homework, Hugging Face also provides tools for fine-tuning.\n\n**®[5] Task:  It's time to Hang 10**\nRun the** **question and answer fine-tuning notebook from [The Big Table of Tasks](https://huggingface.co/transformers/examples.html?highlight=big%20table%20tasks#the-big-table-of-tasks) and run it. In the cell below, provide the final exact match and f-1 scores and explain what they mean.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 16"]}]}