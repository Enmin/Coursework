{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw9-bert-qa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "jLqs7QZN90_4"
      },
      "source": [
        "### Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "2f08f224-eab0-43b3-b5e6-7961b7b75a4f",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "nmI9vC5L90_-"
      },
      "source": [
        "**Note:** This assignment is due on Sunday, April 25th at Noon ET and will be graded Pass/Fail.  No extensions allowed.\n",
        "\n",
        "Dear Students,\n",
        "\n",
        "Welcome to the last homework in DATA2040-sp21.  You can work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n",
        " \n",
        "\n",
        "The exercises in this homework are designed to help ensure you understand the setup for pre-training a BERT model and how to fine-tune BERT models for specific applications.\n",
        " \n",
        "After completing this assignment, you’ll understand the connection between BERT models and BERT applications and have the know-how to perform fine-tuning for Extractive Question and Answering (EQA) using a Keras head model (running on GPUs and TPUs) and with the standard HuggingFace.co transformer library (running on GPUs).\n",
        " \n",
        "Along the way, you’ll demonstrate your understanding of the data preprocessing methods required for both BERT and EQA and your knowledge of applicable training loss functions and metrics.\n",
        " \n",
        "**Primary Resource**\n",
        "This paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) introduced BERT to the world. Please use it as your primary reference while working on this assignment, i.e., try and work through the relevant parts. \n",
        "\n",
        "Why? Because papers like this one are the format in which most new Deep Learning techniques are introduced. It often takes several years (and sometimes decades) before secondary materials become available for the methods that become recognized as important.\n",
        "\n",
        "**Additional Resources **\n",
        "Once you've taken a shot at it, please feel free to access other materials and google.  Below is a curated list of suggestions.\n",
        "\n",
        "First up is the paper's official github [repo](https://github.com/google-research/bert).  Mapping papers to code, and code to papers is a great way to improve your practical DL skills. The code is well organized, but complex; here is one file that may be particularly helpful to took at  [run_pretraining.py](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_pretraining.py#L131)\n",
        "\n",
        "Next up areJay Alammar's blog posts on Transformers and BERT:\n",
        "\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "- [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
        "\n",
        "\n",
        "**The Future of DL (and Data Science)**\n",
        "Up to now, understanding what's been learned by complex NLP models has been a challenge.  Alammar's Pyhton [Ecco](https://www.eccox.io/) package is one attempt to change that. It provides visualizations of the information in Transformer models. These two blog posts provide an introduction to the work so far:\n",
        "\n",
        "- [Interfaces for Explaining Transformer Language Models](http://jalammar.github.io/explaining-transformers/)\n",
        "- [Find the Words to Say: Hidden State Visualization for Language Models](http://jalammar.github.io/hidden-states/)\n",
        "\n",
        "\n",
        "People that have a deep understanding of complex structures are in a good position to create innovative applications based on them and to perhaps develop new and important structures based on this knowledge.  The development of BERT is a perfect example.  \n",
        "\n",
        "\n",
        "Good luck wrapping up your Final Projects and this assignment.\n",
        " \n",
        "On behalf of the entire course staff, I would like to thank you for participating in DATA 2040. It has been a pleasure to have you all as students.\n",
        "\n",
        "Best Wishes,\n",
        "Dan Potter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "ae320a95-f6ad-48f4-a7c0-661d05c4beef",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "fvot9ono91AA"
      },
      "source": [
        "**Partner Option**\n",
        "You may work with up to 1 additional partner on this assignment; only one partner may submit if you choose to do this. \n",
        "**®Task: **If you worked with a partner on this assignment, please list their official email address below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ETuD_Pe991AB"
      },
      "source": [
        "### Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "f47e8206-12ff-4e1e-83b9-40eb15b997ee",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "qLC0Yd1D91AB"
      },
      "source": [
        "**Partner Option**\n",
        "**®Task:** If you worked with a partner, please confirm that you and your partner both understand that only one of you may submit this homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "LCJl8ng-91AB"
      },
      "source": [
        "### Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "76d1fb4e-3ee7-460f-95cb-0169d64aae47",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "jRJMuR9u91AB"
      },
      "source": [
        "**Pretraining BERT**\n",
        "\n",
        "**®[15] Task: A is for Attention, B is for Bert, C is for Cross-Entropy Loss **\n",
        "Describe how BERT is pretrained.  \n",
        "1. Include an explanation of the what training inputs look like, including the labels that are learned.\n",
        "2. As part of this, include a few specific training input examples.  Include details on any special tokens used (like [CLS], [SEP], [MASK]), and what the corresponding labels to learn are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPzfr0zjK8KV"
      },
      "source": [
        "In general, the BERT is trained using two unsupervised task, Masked Language Modeling and Next Sentence Prediction. The loss used here is Cross Entropy to between the labels and predictions. \n",
        "\n",
        "1. To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\n",
        "The training input data includes BooksCorpus and English WikiPedia text messages. The start of the sentence is with [CLS], masked tokens with [MASK] and seperate between sequences with [SEP]. Also, the padding is used to have fixed length.\n",
        "\n",
        "2. 15% of the word tokens in the sequence will be masked and 80% of the time, the token is masked with special token [MASK], 10% of the time unchanged and 10% of the time with random token to force BERT to learn from the previous and later tokens instead of learning from the word itself. The training use Cross-Entropy Loss.\n",
        "Also for NSP, 50% of the time, the sentences are in order and 50% percent of the time the second sentence is actually not the next one.\n",
        "\n",
        "Masked LM: \\\\\n",
        "The input: [CLS] the man went to [MASK] store [SEP] he bought a bottle of [MASK] coke [SEP] \\\\\n",
        "The output: the diet \\\\\n",
        "The input: [CLS] I love reading [MASK] \\\\\n",
        "The output: books \n",
        "\n",
        "NSP:\\\\\n",
        "The input: [CLS] I am a Brown [MASK] [SEP] I study [MASK] science [SEP] \\\\\n",
        "The output: IsNext \\\\\n",
        "The input: [CLS] The day is [MASK] [SEP] NBA pauses due to [Mask] [SEP] \\\\\n",
        "The output: NotNext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "YZozvkfF91AC"
      },
      "source": [
        "### Problem 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a0433bcf-fdac-4ab7-9c6f-e623cb90530d",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "y2KbjmH391AC"
      },
      "source": [
        "**Pretraining BERT**\n",
        "\n",
        "**®[5] Task: Attention to detailZzzz**\n",
        "How is the loss function defined?  Since BERT is trained on more than one task, how are they combined?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgkOseIVYhBX"
      },
      "source": [
        "The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "B8ZeTgor91AD"
      },
      "source": [
        "### Problem 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "b510a209-203a-4b5c-b0d8-20f06c6639eb",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "cIudSQ7t91AD"
      },
      "source": [
        "**Using pre-trained BERT models**\n",
        "\n",
        "**®[10] Task: Machine Intelligence?**\n",
        "The BERT paper explains how to configure and finetune a pre-trained BERT model to perform various NLP tasks. \n",
        "1. What are the specific tasks described? \n",
        "2. At which tasks was BERT documented as being state of the art?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrKnvXYKY2L6"
      },
      "source": [
        "1. GLUE (General Language Understanding Evaluation), SQuAD v1.1 (Stanford Question Answering Dataset),  SQuAD v2.0 and SWAG (Situations With Adversarial Generations).\n",
        "\n",
        "2. SQuAD v2.0, MNLI (Multi-Genre Natural Language Inference), QQP (Quora Question Pairs), QNLI (Question Natural Language Inference ), SST-2 (The Stanford Sentiment Treebank), CoLA (The Corpus of Linguistic Acceptability), STS-B (The Semantic Textual Similarity Benchmark), MRPC (Microsoft Research Paraphrase Corpus), RTE (Recognizing Textual Entailment), WNLI (Winograd NLI), SWAG (Situations With Adversarial Generations). In total, BERT advanced 11 NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "h2dmodyb91AD"
      },
      "source": [
        "### Problem 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e7f61f66-dbaf-4fa2-bcfd-78fce60e2630",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "llkc25Tj91AE"
      },
      "source": [
        "**Using pre-trained BERT models**\n",
        "\n",
        "**®[5] Task: School versus learning a new task**\n",
        "Which takes longer, pre-training or fine-tuning? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyeFIrBXdYMn"
      },
      "source": [
        "Pre-training. Since fine-tuning of BERT is sensitive to small datasets than large-scaled datasets, and only a few parameters will be fine-tuned during the process. Also, hyperparameters are chosen in a given range with most of them same as those in pre-training. Furthermore, fine-tuning only cares about a specific task and uses features generated from pre-training to solve speicfic problems. Therefore, fine-tuning is faster than pre-training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "_j-xplsE91AE"
      },
      "source": [
        "### Problem 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e5061573-2707-4d57-a78b-ff48d9e1e852",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "XtLB-w1W91AE"
      },
      "source": [
        "**Extractive question and answering**\n",
        "Please review the following extractive question and answer (EQA) problems before beginning to answer them.  Each one will be graded on the information requested. \n",
        "\n",
        "**®[15] Task: Focus on the answer**\n",
        "1. What is extractive question and answering? \n",
        "2. What is [SQuAD1.1](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/)? \n",
        "3. Provide an example of the task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPriheiveS5V"
      },
      "source": [
        "1. Given a question and passage, the model should predict the answer or provide the answer text to that question.\n",
        "\n",
        "2. SQuAD1.1 is a collection of 100k crowdsourced question/answer pairs. The answer in the label is directly cutted from a part of the passages. Three ground truth answers will usually contain three different labels (same if no enough).\n",
        "\n",
        "3. Question: What is a course of study called? \\\\\n",
        "Passage: The role of teacher is often formal and ongoing, carried out at a school or other place of formal education. In many countries, a person who wishes to become a teacher must first obtain specified professional qualifications or credentials from a university or college. These professional qualifications may include the study of pedagogy, the science of teaching. Teachers, like other professionals, may have to continue their education after they qualify, a process known as continuing professional development. Teachers may use a lesson plan to facilitate student learning, providing a course of study which is called the curriculum.\\\\\n",
        "Ground Truth Answers: the curriculum. curriculum curriculum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "bdihoyNO91AE"
      },
      "source": [
        "### Problem 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "8d3b86f3-ae46-4125-bf1c-2fbacb3959d8",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "rSf1lFTu91AE"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®Task [10]: Teaching BERT to do EQA #1**\n",
        "Describe the BERT fine-tuning process for extractive question and answering.  Assume the SQuAD1.1 dataset is being used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxEKnzNKhKGu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "mVofddtW91AF"
      },
      "source": [
        "### Problem 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a0590c33-dc37-4390-9d4c-c6cfec034619",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "9ryJNwG891AF"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[10] Task: Teaching BERT to do EQA #2**\n",
        "Provide two training input examples, and details on [CLS], [SEP], [MASK], and labels. It would be enough if the format is correct. \n",
        "(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "tZpLNais91AF"
      },
      "source": [
        "### Problem 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "a7a44d60-ccb5-47af-b4be-7069d32ad1bc",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "7oI7JdY291AF"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[10] Task: Teaching BERT to do EQA #3**\n",
        "Confirm your training sample input understanding by running them through the the preprocessing code in this [notebook](https://keras.io/examples/nlp/text_extraction_with_bert/) (keras.io). \n",
        "\n",
        "Copy and paste the output of your computer experiment below and explain it. (Hint: you may find that the paper and the code change the order of some things, that is OK)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "7e1kiFJS91AG"
      },
      "source": [
        "### Problem 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "e6ba1d0e-ff4b-47a2-ae40-33aae7cfa207",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "nDipJy7h91AG"
      },
      "source": [
        "**Extractive question and answering**\n",
        "**®[5] Task: Teaching BERT to do EQA #4**\n",
        "The pre-trained BERT model has multiple outputs; which ones are used for fine-tuning the squad task and which ones are ignored?\n",
        "(Hint: you can find relevant information in the [BERT](https://arxiv.org/pdf/1810.04805.pdf) paper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "kpCPaf0291AG"
      },
      "source": [
        "### Problem 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "4545897b-b70d-4b85-9ffc-fd62cdc0c6d8",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "Y0DMGv0k91AG"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®[5] Task: Teaching BERT to do EQA #5**\n",
        "Identify and describe the loss function that is used. Provide sufficient detail for the reader to understand why it’s appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "9bFV2cA591AH"
      },
      "source": [
        "### Problem 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "206eeb8f-9039-4938-9a50-3ff8b707acb9",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "pLtEN0I991AH"
      },
      "source": [
        "**Extractive question and answering**\n",
        "The keras.io notebook includes code to take advantage of TPU’s. It requires the model weights to be represented explicitly in the tensorflow model graph. \n",
        "\n",
        "**®[5] Task: Run with the wind #1**\n",
        "Copy and paste the code cell(s) that expose the head model and compile it within a TPU context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "crFHiA0N91AH"
      },
      "source": [
        "### Problem 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "5b37fcca-6b19-4d45-84ee-50073227c494",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "lYv49ROP91AH"
      },
      "source": [
        "**Extractive question and answering**\n",
        "\n",
        "**®[5] Task: Run with the wind #2 **\n",
        "Using a batch size of 16, try fine-tuning with TPU vs. GPU enabled. Report the speed change below. How much GPU memory is used with a batch size of 16?  \n",
        "\n",
        "Hints:\n",
        "1. If you setup wandb.ai before you begin your experiments, you can use the System panel  ![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-76da31d9-93b6-49ae-a0ca-356e26e85e8f.png?alt=media&token=205261cb-6984-4246-b866-938082d192c1) to display system resource usage over time.\n",
        "2. GPUs are slower than TPUs for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ZbKOcmrm91AH"
      },
      "source": [
        "### Problem 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "628736ea-ead0-4967-bba8-d7357ded5147",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "c9wKblgH91AI"
      },
      "source": [
        "**HuggingFace in practice**\n",
        "\n",
        "Hugging face[ pipelines](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) are an effortless way to use pre-trained and pre-finetuned models. As we saw in the last homework, Hugging Face also provides tools for fine-tuning.\n",
        "\n",
        "**®[5] Task:  It's time to Hang 10**\n",
        "Run the** **question and answer fine-tuning notebook from [The Big Table of Tasks](https://huggingface.co/transformers/examples.html?highlight=big%20table%20tasks#the-big-table-of-tasks) and run it. In the cell below, provide the final exact match and f-1 scores and explain what they mean.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ex0Op6Je91AI"
      },
      "source": [
        "### Problem 16"
      ]
    }
  ]
}