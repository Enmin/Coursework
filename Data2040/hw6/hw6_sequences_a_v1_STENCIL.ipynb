{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw6_sequences_a_v1_STENCIL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b215b822c8444fd8be6670b5cbbe4cf4":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3d0e9cb7e95344989fe212bcad1abca0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_552ff3e02a294a178e922fdb844863a4","IPY_MODEL_0c1a243e5c504db48c0d31aabe1fceba"]}},"3d0e9cb7e95344989fe212bcad1abca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"552ff3e02a294a178e922fdb844863a4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","state":{"_view_name":"LabelView","style":"IPY_MODEL_ce92453238354d11a1277e141850edac","_dom_classes":[],"description":"","_model_name":"LabelModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0.41MB of 0.41MB uploaded (0.00MB deduped)\r","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3cd9e38b37a24bdca23c29e8ae7af0a1"}},"0c1a243e5c504db48c0d31aabe1fceba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fd930db1046f44e98dbf4840fd44e7e6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21d78145570844e9920104b249c439bb"}},"ce92453238354d11a1277e141850edac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3cd9e38b37a24bdca23c29e8ae7af0a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd930db1046f44e98dbf4840fd44e7e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"21d78145570844e9920104b249c439bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"bZfl2HO8S9Lb"},"source":["# hw6-sequences-a\n","\n","The goal of this assignment is to develop a simple next-character prediction using a traning corpus, and then to leverage that model to generate strings of characters."]},{"cell_type":"code","metadata":{"id":"ILMIRUSJV68A"},"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import os\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-PiifXNVXIk"},"source":["In this assignment we will work with a dataset of Elon Musk's tweets. The data comes from this [Kaggle Dataset](https://www.kaggle.com/vidyapb/elon-musk-tweets-2015-to-2020?select=elonmusk.csv), and you can download it from this [link](https://raw.githubusercontent.com/beckyleii/data-bucket/master/elonmusk.csv).  **Optional:** Check out the EDA notebooks created for this dataset. The one called \"NLP with TextHero NLP with TextHero | EDA with SweetViz\" was created by the person who added this dataset to Kaggle includes example code for scraping twitter data.\n","\n","### Task ETL\n","\n","Using the code cell below write some ETL code to load the data using the link above, and store it in memory as a single (very long) string. The data is small enough to do this. **Note:** When working with a larger [text corpus](https://en.wikipedia.org/wiki/Text_corpus), the ETL would need to store the intermediates to disk). \n","\n","Join individual tweets with a new line character (\"\\n\") or some other special character.  In many applications, some additional filtering and transformations character (or word) might be applied at this step, but there is no need to do so in this assignment. "]},{"cell_type":"code","metadata":{"id":"X1ZybqAfV3lK"},"source":["## BEGIN SOLUTION Task ETL -- 3 lines of code\n","\n","\n","\n","## END SOLUTION Task 1\n","\n","vocab = sorted(list(set(text)))\n","vocab_size = len(vocab)\n","assert vocab_size == 364"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LFjSVAlWzf-N"},"source":["## Text as numbers, part I\n","\n","To train a neural net using gradient descent, we need text as numbers!  Sounds reasonable - but how? \n","\n","Unfortunately.  There isn't a definitive way to do this.  One simple approach is to break the text into a list of tokens (character-by-character in this assignment).  And then assign a numerical representation to each token.\n","\n","Since we'll be wanting to go back and forth between the tokens and their numerical representation, it is helpful to create an inverse mapping of this assignment as well."]},{"cell_type":"markdown","metadata":{"id":"vlTEwzHlW4Ey"},"source":["### Task `token2nrep` and `nrep2token`\n","Create two dictionaries: one mapping vocabulary characters to numbers, named `token2nrep`, and another from numbers to tokens, named `nrep2token`. \n","\n","*Python pro-tip:* `enumerate` is a lovely construct\n","```\n","for index, value in enumerate(L):\n","    # do something\n","```\n","\n","*Python pro-tip:* dictionary comprehensions are a thing!\n","```\n","char2idx = {??? for x in L)\n","```\n"]},{"cell_type":"code","metadata":{"id":"IalZLbvOzf-F","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1953502d-8ae4-47f5-e639-d1659ef40a7b"},"source":["## BEGIN SOLUTION - combine the pro-tips and try for a 2 line solution! \n","\n","\n","## END SOLUTION Task 2.1\n","\n","assert nrep2token[token2nrep['a']]=='a'\n","assert len(token2nrep) == len(vocab)\n","\n","for char in 'Elon':\n","  nrep = token2nrep[char]\n","  print(f\"{char} -> {nrep:3} -> {nrep2token[nrep]}\")\n","\n","def text2nrep(s):\n","  return np.array([token2nrep[c] for c in s])\n","\n","def nrep2text(nrep):\n","  return ''.join([nrep2token[n] for n in nrep])\n","\n","s=\"Elon\"\n","print(f'\"{s}\" -> {text2nrep(s)} -> \"{nrep2text(text2nrep(s))}\"')  \n","\n","assert nrep2text(text2nrep(\"Elon\")) == \"Elon\"\n","assert len(text2nrep(text)) == len(text)\n","assert nrep2text(text2nrep(text)) == text\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["E ->  38 -> E\n","l ->  75 -> l\n","o ->  78 -> o\n","n ->  77 -> n\n","\"Elon\" -> [38 75 78 77] -> \"Elon\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hgsVvVxnymwf"},"source":["### Create training examples and targets with tf.data\n","\n","In this task, we will write code that converts text into training data - each training example will contain a sequence of input characters and and a target sequence characters, both represented in numerical form.\n","\n","The model we are creating will predict the target sequence from each input sequence.  For the setup in this assignment, the target will contain the same token sequence as the input, except it will be shifted one token to the right.  The model requires fixed length input and output sequences.\n","\n","The approach we will take is to break the text into chunks of `seq_length+1` characters, and then split them into an input and target, each of length `seq_length`.\n","\n","For example, if the input text is \"Trump is done.\", and `seq_length` is 3, the input and target sequences would have nrep2text to text representatuibs for th following input, target sequnce pairs:\n","\n","\n","```\n","\"Tru\", \"rum\"\n","\"rum\", \"ump\"\n","\"ump, \"mp \"\n","\"mp , \"p i\"\n","\"p i, \" is\"\n","...\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"EdU-Eh605NQk"},"source":["### Task: Text Dataset design\n","The tf.data api provides a scalable way to to this. You need to:\n","\n","1. Create a `Dataset` of `text2nrep('text')` using the `from_tensor_slices` constuctor.\n","2. Use the `window` method to configure a 'dataset of datasets', where each window returned is **always** of length `seq_length+1`. (see `drop_remanider`)\n","3. Use `flat_map` and the provided `sub_to_batch` function to flatten the window dataset of datasets into a sequential data set containing sequential overlapping windows of text.\n","4. Use `map` method, and the `split_input_target` method to split these sequences into appropriate (X input, y target) pairs"]},{"cell_type":"code","metadata":{"id":"0UHJDA39zf-O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"763d7e8a-ea17-4078-8b4f-e6c627a950e2"},"source":["def create_seq_data(corpus, text2nrep, seq_length):\n","    def sub_to_batch(sub):\n","        return sub.batch(seq_length+1, drop_remainder=True)\n","\n","    def split_input_target(seq):\n","        input_seq = seq[:-1]\n","        target_seq = seq[1:]\n","        return input_seq, target_seq\n","\n","    ## BEGIN SOLUTION -- 3 lines of code\n"," \n","\n"," \n","    ## END SOLUTIONS\n","    return seq\n","\n","dataset = create_seq_data(\"Trump is done.\", text2nrep, seq_length=3)\n","for it in dataset.take(5):\n","  print([nrep2text(it[0].numpy()), nrep2text(it[1].numpy())])\n","print('...')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Tru', 'rum']\n","['rum', 'ump']\n","['ump', 'mp ']\n","['mp ', 'p i']\n","['p i', ' is']\n","...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h9YYD-Ms7FwS"},"source":["The output above should match the following:\n","```\n","['Tru', 'rum']\n","['rum', 'ump']\n","['ump', 'mp ']\n","['mp ', 'p i']\n","['p i', ' is']\n","```"]},{"cell_type":"markdown","metadata":{"id":"e4BRLvge7Z-Q"},"source":["### Task: Batching with `tf.data`\n","\n","Below is some code to shuffle and batch using `create_seq_data`. Answer the following in Prismia:\n","\n","1. How many batches of data are created per epoch? Is any data unused? Explain.\n","2. More generally, add an explanation of what the parameters SEQ_LENGTH, BUFFER_SIZE, BATCH_SIZE are doing.\n","3. Below we are shuffling and then batching. Explain what happens if instead, you shuffle after you batch. \n","4. In general, when training, should you shuffle and then batch, or batch and then shuffle? \n","\n","hints: Use `nrep2text` to get a clearer picture of what is happening.  If you are still confused, try uncommenting the the alternate `# test_text` variable.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbYSSaoMiz_r","outputId":"1e1d1c93-4e35-4f16-a23a-3ec84dc946d3"},"source":["SEQ_LENGTH = 30\n","BUFFER_SIZE = 2\n","BATCH_SIZE = 4\n","test_text = \"The quick brown fox jumps over the lazy dog..\"\n","# test_text = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRS'\n","print(test_text)\n","\n","dataset = create_seq_data(test_text, text2nrep, seq_length=SEQ_LENGTH)\n","batched_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","print(batched_dataset)\n","for X, y in batched_dataset:\n","  print(f\"X = \\n{X}\")\n","  print(f\"y = \\n{y}\")\n","\n","print(f'X.shape = {X.shape}')\n","print(f'y.shape = {y.shape}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The quick brown fox jumps over the lazy dog..\n","<BatchDataset shapes: ((4, 30), (4, 30)), types: (tf.int64, tf.int64)>\n","X = \n","[[71 68  1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82\n","   1 78 85 68 81  1]\n"," [68  1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1\n","  78 85 68 81  1 83]\n"," [53 71 68  1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79\n","  82  1 78 85 68 81]\n"," [ 1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78\n","  85 68 81  1 83 71]]\n","y = \n","[[68  1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1\n","  78 85 68 81  1 83]\n"," [ 1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78\n","  85 68 81  1 83 71]\n"," [71 68  1 80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82\n","   1 78 85 68 81  1]\n"," [80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85\n","  68 81  1 83 71 68]]\n","X = \n","[[80 84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85\n","  68 81  1 83 71 68]\n"," [84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68\n","  81  1 83 71 68  1]\n"," [66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1\n","  83 71 68  1 75 64]\n"," [74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83\n","  71 68  1 75 64 89]]\n","y = \n","[[84 72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68\n","  81  1 83 71 68  1]\n"," [72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81\n","   1 83 71 68  1 75]\n"," [74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83\n","  71 68  1 75 64 89]\n"," [ 1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71\n","  68  1 75 64 89 88]]\n","X = \n","[[72 66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81\n","   1 83 71 68  1 75]\n"," [ 1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71\n","  68  1 75 64 89 88]\n"," [65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71 68\n","   1 75 64 89 88  1]\n"," [81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71 68  1\n","  75 64 89 88  1 67]]\n","y = \n","[[66 74  1 65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1\n","  83 71 68  1 75 64]\n"," [65 81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71 68\n","   1 75 64 89 88  1]\n"," [81 78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71 68  1\n","  75 64 89 88  1 67]\n"," [78 86 77  1 69 78 87  1 73 84 76 79 82  1 78 85 68 81  1 83 71 68  1 75\n","  64 89 88  1 67 78]]\n","X.shape = (4, 30)\n","y.shape = (4, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MecZD1C9Eo5R","outputId":"f06435ae-9ba2-4a75-f6e6-e297fdabec1c"},"source":["# BEGIN Hint Solution\n","\n","\n","\n","# END Hint Solution"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<BatchDataset shapes: ((4, 30), (4, 30)), types: (tf.int64, tf.int64)>\n","X = \n","['The quick brown fox jumps over', 'e quick brown fox jumps over t', ' quick brown fox jumps over th', 'he quick brown fox jumps over ']\n","y = \n","['he quick brown fox jumps over ', ' quick brown fox jumps over th', 'quick brown fox jumps over the', 'e quick brown fox jumps over t']\n","X = \n","['quick brown fox jumps over the', 'ick brown fox jumps over the l', 'uick brown fox jumps over the ', 'ck brown fox jumps over the la']\n","y = \n","['uick brown fox jumps over the ', 'ck brown fox jumps over the la', 'ick brown fox jumps over the l', 'k brown fox jumps over the laz']\n","X = \n","['k brown fox jumps over the laz', 'brown fox jumps over the lazy ', 'rown fox jumps over the lazy d', ' brown fox jumps over the lazy']\n","y = \n","[' brown fox jumps over the lazy', 'rown fox jumps over the lazy d', 'own fox jumps over the lazy do', 'brown fox jumps over the lazy ']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y2QCPdXDJCLo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MotDDvOilwG3"},"source":["## A simple parameterized RNN model\n","\n","Below we define a simple parameterized RNN model:\n","\n","* the first layer is an keras [embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n","\n","* the second layer can be either a keras [GRU layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) or [LSTM layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) - or both!\n","\n","The model is said to be **paramterized** because it many of its parameters are passed in via parameter dictionary `p`.\n","\n","In general, storing experimental parameters, including model hyper-parameters in  dictionaries is a great way to keep track of your experiments."]},{"cell_type":"code","metadata":{"id":"-LIq-w4wl24C"},"source":["def build_model(**p):\n","  tf.keras.backend.clear_session()\n","  m = tf.keras.Sequential()\n","  m.add(tf.keras.layers.InputLayer(\n","           input_shape=(p['SEQUENCE_length'],) \n","          ,batch_size=p['BATCH_size']\n","          ))\n","  m.add(tf.keras.layers.Embedding(\n","           input_length=10\n","          ,input_dim = p['VOCAB_size']\n","          ,output_dim = p['EMBEDDING_dim']\n","          ))\n","    \n","  if p['GRU_units'] > 0:\n","    m.add(tf.keras.layers.GRU(\n","             units = p['GRU_units']\n","            ,return_sequences=True\n","            ,stateful=True\n","            ,recurrent_initializer='glorot_uniform'\n","            ))\n","\n","  if p['LSTM_units'] > 0:\n","    m.add(tf.keras.layers.GRU(\n","             units = p['LSTM_units']\n","            ,return_sequences=True\n","            ,stateful=True\n","            ,recurrent_initializer='glorot_uniform'\n","            ))\n","\n","  m.add(tf.keras.layers.Dense(p['VOCAB_size']))\n","  \n","  m.compile(\n","       optimizer='adam'\n","      ,loss=tf.keras.losses.SparseCategoricalCrossentropy(\n","          from_logits=True)\n","      ,metrics=['accuracy']\n","      )\n","\n","\n","  return m\n","\n","p = {\n","     'EMBEDDING_dim':100\n","    ,'GRU_units':1024\n","    ,'LSTM_units':0\n","    ,'VOCAB_size':vocab_size\n","    ,'BUFFER_size':1000\n","    ,'SEQUENCE_length':100\n","    ,'BATCH_size':32\n","    ,'BATCH_per_epoch':100\n","    ,'CORPUS_fraction':.01\n","    }\n","\n","model = build_model(**p) \n","# model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iF4zaa4hLQyJ"},"source":["## A simple parameterized data pipeline\n","\n","The `setup_training_dataset` function that returns a training dataset appropriate for the RNN model.  It uses the same approach to parameteization as we did in `build_model`."]},{"cell_type":"code","metadata":{"id":"XDjMQ8FpAqg8"},"source":["def setup_training_dataset(**p):\n","  dataset = create_seq_data(text[:int(p['CORPUS_fraction']*len(text))], text2nrep, seq_length=p['SEQUENCE_length'])\n","  training_dataset = dataset.shuffle(p['BUFFER_size']).batch(p['BATCH_size'], drop_remainder=True).take(p['BATCH_per_epoch'])\n","  return training_dataset\n","\n","for input_example_batch, target_example_batch in setup_training_dataset(**p):\n","    example_batch_predictions = model(input_example_batch)\n","    assert example_batch_predictions.shape == (p['BATCH_size'], p['SEQUENCE_length'], len(vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7p1aRs7BgIA"},"source":["### Task Understanding Parameterized Models\n","Add a comment for each parameter in the parameter dictionary above, paste a copy to Prismia as well. Your answer should look something like this:\n","\n","```\n","p = {\n","     'EMBEDDING_dim':100     # short comment\n","    ,'GRU_units':1024        # short comment\n","    ,'LSTM_units':0          # ...\n","    ,'VOCAB_size':vocab_size # \n","    ,'BUFFER_size':1000      #\n","    ,'SEQUENCE_length':100   #\n","    ,'BATCH_size':32         #\n","    ,'BATCH_per_epoch':100   #\n","    ,'CORPUS_fraction':.01   #\n","    }\n","```"]},{"cell_type":"markdown","metadata":{"id":"-boFPQPiKPbV"},"source":["# Fit model and generate fake Elon tweets\n","Now that everying is nicely setup, let's fit the model and generate some fake tweeks."]},{"cell_type":"markdown","metadata":{"id":"9u0zaooSmETa"},"source":["Now that everything is nicely organized let's run our model."]},{"cell_type":"code","metadata":{"id":"Qgpc6l6Uotgj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6726950-b6a5-40d2-c63c-b0b128357795"},"source":["p = {\n","     'EMBEDDING_dim':100\n","    ,'GRU_units':1024\n","    ,'LSTM_units':0\n","    ,'VOCAB_size':vocab_size\n","    ,'BUFFER_size':1000\n","    ,'SEQUENCE_length':100\n","    ,'BATCH_size':32\n","    ,'BATCH_per_epoch':100\n","    ,'CORPUS_fraction':.01\n","    , 'EPOCHS':10\n","}\n","model = build_model(**p)\n","model.fit(setup_training_dataset(**p), epochs=p['EPOCHS'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","100/100 [==============================] - 5s 37ms/step - loss: 4.2756 - accuracy: 0.1314\n","Epoch 2/10\n","100/100 [==============================] - 4s 37ms/step - loss: 2.5952 - accuracy: 0.2802\n","Epoch 3/10\n","100/100 [==============================] - 4s 37ms/step - loss: 2.0400 - accuracy: 0.4079\n","Epoch 4/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.8745 - accuracy: 0.7746\n","Epoch 5/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.2701 - accuracy: 0.9497\n","Epoch 6/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1997 - accuracy: 0.9613\n","Epoch 7/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1746 - accuracy: 0.9644\n","Epoch 8/10\n","100/100 [==============================] - 4s 39ms/step - loss: 0.1621 - accuracy: 0.9665\n","Epoch 9/10\n","100/100 [==============================] - 4s 39ms/step - loss: 0.1521 - accuracy: 0.9674\n","Epoch 10/10\n","100/100 [==============================] - 4s 39ms/step - loss: 0.1463 - accuracy: 0.9689\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f27563ad510>"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"markdown","metadata":{"id":"GOqAwkl4qzoF"},"source":["## A simple generative approach\n","\n","The `generate text` function below creates next token predictions.  It is fun to play with!\n","\n","### Task: [Tabula rasa](https://en.wikipedia.org/wiki/Tabula_rasa)\n","Try running the cell below with the set_weights commented out and also uncommented.  Explain what you see in Prismia.  Include copy-and-paste examples of both runs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BxmSW4PRPT2D","outputId":"2f40d339-2c3e-49f4-eabe-496f24c29f29"},"source":["# Generating text using the learned model\n","\n","def generate_text(model, start_string, count = 1000):\n","  # Note: Because of the way the RNN state is passed from timestep to \n","  # timestep, the model only accepts a fixed batch size once built.\n","  # To run the model with a different `batch_size`, we need to rebuild \n","  # the model to accomodate a batch-size of 1. \n","\n","  gmodel = tf.keras.models.clone_model(model, input_tensors = tf.keras.Input(batch_input_shape=(1,100)))\n","  gmodel.set_weights(model.get_weights())\n","  input_seq = tf.expand_dims(text2nrep(start_string), 0) # add batch dimension\n","  \n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  temperature = 1.0 # 0.1 is very ridgid; 10 is very noisy\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  for i in range(count):\n","      predictions = gmodel(input_seq)\n","      predictions = tf.squeeze(predictions, 0) # remove batch dimension\n","      \n","      # using a categorical distribution to predict the character returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()      \n","      text_generated.append(nrep2token[predicted_id])\n","\n","      # Use the predicted nrep as the next input to the model\n","      input_seq = tf.expand_dims([predicted_id], 0) # add batch dimesion\n","\n","  return (start_string + ''.join(text_generated))\n","  \n","print(generate_text(model, start_string=u\"Wild times! \", count=1000))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Wild times! xthe furutheas so surreal, but the negative propaganda is still all out there & easy to find in social media & press interviews, so it’s not just our imagination!\n","Make sure to read ur terms & conditions before clicking accept!\n","Samwise Gamgee\n","Altho Dumb and Dumber is 🔥🔥\n","Progress update August 28\n","Sure\n","If you can’t beat em, join em\n","Neuralink mission statement\n","Tesla China team is awesome!\n","Words are a very lossy compression of thought\n","If you get past Mars, the asteroids, moons of Jupiter & Saturn, inevitably you reach Uranus!\n","🖤✨Carl Sagan ✨🖤\n","Essentially. Long-term purpose of my Tesla stock is to help make life multiplanetary to ensure it’s continuance. The massive capital needs are in 10 to 20 years. By then, if we’re fortunate, Tesla’s goal of acceleratan’t.\n","AI symbiosis while u wait\n","There’s some of that too\n","True, it sounds so surreal, but the negative propaganda is still all out there & easy to fying (finally).\n","True\n","Wow, IHOP & GitHub are close\n","Best use of the term “Full Stack”?\n","For sure.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pBlN14IaueG0"},"source":["## Experiments\n","\n","All the organizational work but paramterizing parts of this notebook will allow you to easily and methodically experiment with different versions of the model.  \n","\n","Weights and Baises is a user-friendly version of TensorBoard.dev. Try running the cell below to set it up.  And use the last cell in this notebook try out different parameters.  Clicking the link will let you see all of your training runs, including the parameters that used.   You can even group by different parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"1XpWJDAxJWKQ"},"source":["## Weights and Biases \n","Weight and biases provides an easy nice way to track experminets.  Below is the setup code needed.  They provide free accounts, and additional provisioning for .edu acccount.  The first time you run block you will have to authenticate in a manner similar to the way you authenticate a google drive."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHRepfzkUo8T","outputId":"4e33a2ed-814a-409b-cfbc-cd36cd4bb69a"},"source":["!pip -q install wandb\n","import wandb\n","wandb.login()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"markdown","metadata":{"id":"Z68SnQzknq13"},"source":["\n","### Task: Research Question\n","The synthetic tweets results are very impressive when you first see them. But if you re-run the generate_text multiple times, they start to seem very familiar.  Try and figure out why this is the case, then try and figure out a way to improve the results.  \n","\n","Document your efforts in the last Prismia problem.  Do not spend more than a couple of hours on this.  \n","\n","If you create some impressive or particularly funny output, please consider posting to the Piazza hw6 [Fake Tweet Fun](https://piazza.com/class/kjj6m8xbzbp141?cid=238) thread.  \n","\n","We are much more interested in you exploring the experimental setup and parameter approach available used in this notebook, than in creating a better solution to this toy problem. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b215b822c8444fd8be6670b5cbbe4cf4","3d0e9cb7e95344989fe212bcad1abca0","552ff3e02a294a178e922fdb844863a4","0c1a243e5c504db48c0d31aabe1fceba","ce92453238354d11a1277e141850edac","3cd9e38b37a24bdca23c29e8ae7af0a1","fd930db1046f44e98dbf4840fd44e7e6","21d78145570844e9920104b249c439bb"]},"id":"2rapoatpZ0Tg","outputId":"fd91c075-1fdc-4422-c86a-123fc812fb8b"},"source":["p = {\n","     'EMBEDDING_dim':100\n","    ,'GRU_units':1024\n","    ,'LSTM_units':0\n","    ,'VOCAB_size':vocab_size\n","    ,'BUFFER_size':1000\n","    ,'SEQUENCE_length':100\n","    ,'BATCH_size':32\n","    ,'BATCH_per_epoch':100\n","    ,'CORPUS_fraction':.01\n","    ,'EPOCHS':10\n","}; \n","\n","wandb.init(project=\"hw6-sequence-a\", config=p)\n","model = build_model(**p)\n","model.fit(setup_training_dataset(**p), epochs=p['EPOCHS'], callbacks=[wandb.keras.WandbCallback()])\n","tweets = generate_text(model, start_string=u\"Wild times! \", count=1000)\n","wandb.log({'fake-tweets':wandb.Table(data=[[tweets]], columns=[\"tweets\"])})\n","print(f'\\n{tweets}\\n')\n","wandb.finish()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.22<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">pious-bush-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/dfp/hw6-sequence-a\" target=\"_blank\">https://wandb.ai/dfp/hw6-sequence-a</a><br/>\n","                Run page: <a href=\"https://wandb.ai/dfp/hw6-sequence-a/runs/3iuf2qe1\" target=\"_blank\">https://wandb.ai/dfp/hw6-sequence-a/runs/3iuf2qe1</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20210319_034049-3iuf2qe1</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Epoch 1/10\n","100/100 [==============================] - 5s 39ms/step - loss: 4.0758 - accuracy: 0.1306\n","Epoch 2/10\n","100/100 [==============================] - 4s 38ms/step - loss: 2.6772 - accuracy: 0.2710\n","Epoch 3/10\n","100/100 [==============================] - 4s 38ms/step - loss: 2.2404 - accuracy: 0.3481\n","Epoch 4/10\n","100/100 [==============================] - 4s 38ms/step - loss: 1.4176 - accuracy: 0.6029\n","Epoch 5/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.4005 - accuracy: 0.9174\n","Epoch 6/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.2071 - accuracy: 0.9611\n","Epoch 7/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1787 - accuracy: 0.9645\n","Epoch 8/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1632 - accuracy: 0.9666\n","Epoch 9/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1551 - accuracy: 0.9678\n","Epoch 10/10\n","100/100 [==============================] - 4s 38ms/step - loss: 0.1470 - accuracy: 0.9689\n","\n","Wild times! we’re fortunate, Tesla’s goal of accelerating sustainable energy & autonomy will be mostly accomplished.\n","Thank goodness for modern medicine!\n","For sure\n","Coming soon, our battle with Big Tequila! It’s real.\n","That would be next-level 🤣🤣\n","I bought a pair of XL\n","Also true. Haha you ! to ensure it’s continuance. The massive capital needs are in 10 to 20 years. By then, if we’re fortunate, Tesla’s goal of accelerating sustainable elerty m65 years without war. That’s the amazing part.\n","Yeah!\n","Lord of the Rings\n","Looks cooo\n","Thant wall Stank goodn’s real.\n","👀\n","Itt part haha\n","Yes, in plan. Superchargers and public high power wall connectors will keep growing exponentially every year.\n","👀\n","I think so\n","Doing range testing now. Number wall connectors will keep growing exponentially every year.\n","👀\n","I think so\n","Doing range testing now. Number will be significantly higher than 300. Extremely good for any EV, especially an SUV.\n","We have reed continatally an SUV.\n","Ever pay out, as the stock can’t be sold!\n","Nailed it\n","Thanks Joh\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<br/>Waiting for W&B process to finish, PID 6527<br/>Program ended successfully."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b215b822c8444fd8be6670b5cbbe4cf4","version_minor":0,"version_major":2},"text/plain":["VBox(children=(Label(value=' 0.18MB of 0.18MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find user logs for this run at: <code>/content/wandb/run-20210319_034049-3iuf2qe1/logs/debug.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Find internal logs for this run at: <code>/content/wandb/run-20210319_034049-3iuf2qe1/logs/debug-internal.log</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<h3>Run summary:</h3><br/><style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    </style><table class=\"wandb\">\n","<tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.14953</td></tr><tr><td>accuracy</td><td>0.9686</td></tr><tr><td>_runtime</td><td>47</td></tr><tr><td>_timestamp</td><td>1616125296</td></tr><tr><td>_step</td><td>10</td></tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<h3>Run history:</h3><br/><style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    </style><table class=\"wandb\">\n","<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▆▅▃▁▁▁▁▁▁</td></tr><tr><td>accuracy</td><td>▁▂▃▆██████</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr></table><br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 1 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","                    <br/>Synced <strong style=\"color:#cdcd00\">pious-bush-9</strong>: <a href=\"https://wandb.ai/dfp/hw6-sequence-a/runs/3iuf2qe1\" target=\"_blank\">https://wandb.ai/dfp/hw6-sequence-a/runs/3iuf2qe1</a><br/>\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]}]}