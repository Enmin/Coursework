{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw6-sequences-a (mini-assignment) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "xa1tc5Of6Nob"
      },
      "source": [
        "### Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "b53abb39-6b5f-4ebd-93e2-40f9e2182fec",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "wkRgWyeg6Nof"
      },
      "source": [
        "![](https://firebasestorage.googleapis.com/v0/b/prismia.appspot.com/o/user-images%252Fimage-84dabff3-51aa-477c-9201-756f22ed4d88.png?alt=media&token=abc957e0-5eb5-4883-b9ba-8cee42143d1a)\n",
        "\n",
        "Using this [colab-notebook stencil](https://drive.google.com/file/d/1NcWzchZFKh_yQLRQ-tzcakp5ocPOXVNs/view?usp=sharing), you will create a simple character-based tweet generation model.\n",
        "\n",
        "Instructions:  There is a Prismia problem corresponding to each **Task** in the notebook, for each one \n",
        "1. Provide a copy and paste your completed code cell(s).  \n",
        "2. Then provide a written summary of how the code cell(s) operates.\n",
        "3. Include relevant notes (and where appropriate example output) of its operation with respect to the supplied dataset. (You may use multiple code cells to describe this last step)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "c319c92e-d5b6-4909-bf62-2a8e1e1bf1c2",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "AmZr1zH66Nog"
      },
      "source": [
        "® Here is a course staff accessible link to my completed notebook:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruSlK-fk64qh"
      },
      "source": [
        "https://drive.google.com/file/d/1cxqg3gg8Bi-Qj4CMU5c3dKtpNHmx92MO/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "ntH_HGvr6Nog"
      },
      "source": [
        "### Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "131d594d-8b4c-4c41-80b2-52833dbc39f0",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "uxcx4r0F6Noh"
      },
      "source": [
        "### ®[5] Task ETL\n",
        "Using the code cell below write some ETL code to load the data using the link above, and store it in memory as a single (very long) string. The data is small enough to do this. ****Note:**** When working with a larger [text corpus](https://en.wikipedia.org/wiki/Text_corpus), the ETL would need to store the intermediates to disk). \n",
        "\n",
        "Join individual tweets with a new line character (\"\\n\") or some other special character.  In many applications, some additional filtering and transformations character (or word) might be applied at this step, but there is no need to do so in this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVxiST2Z9O8N"
      },
      "source": [
        "## BEGIN SOLUTION Task ETL -- 3 lines of code\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/beckyleii/data-bucket/master/elonmusk.csv\")\n",
        "tweets = np.array(data['tweet'])\n",
        "text = \"\\n\".join(tweets)\n",
        "\n",
        "\n",
        "## END SOLUTION Task 1\n",
        "\n",
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "assert vocab_size == 364"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeSfGkn9TcO"
      },
      "source": [
        "The code read the csv file into dataframe from kaggle url and then take the \"tweet\" column, joining it by \"\\n“ character. There are 364 different characters in this text including number, sign, english and chinese character and emoji. The assert works properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "6ZawZOce6Noh"
      },
      "source": [
        "### Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "80242264-bce9-44d6-b63c-d11874201dee",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "NBrI1zOW6Noh"
      },
      "source": [
        "### ®[5] Task `token2nrep` and `nrep2token`\n",
        "Create two dictionaries: one mapping vocabulary characters to numbers, named `token2nrep`, and another from numbers to tokens, named `nrep2token`. \n",
        "\n",
        "_Python pro-tip:_ `enumerate` is a lovely construct\n",
        "```null\n",
        "for index, value in enumerate(L):\n",
        "    # do something\n",
        "```\n",
        "\n",
        "_Python pro-tip:_ dictionary comprehensions are a thing!\n",
        "```null\n",
        "char2idx = {??? for x in L)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "prismiaParentId": "80242264-bce9-44d6-b63c-d11874201dee",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "uCmVYwcE6Noi"
      },
      "source": [
        "for index, value in enumerate(L):\n",
        "    # do something\n",
        "\n",
        "char2idx = {??? for x in L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bal9M4W4-vO7"
      },
      "source": [
        "## BEGIN SOLUTION - combine the pro-tips and try for a 2 line solution! \n",
        "nrep2token = {index:value for index, value in enumerate(vocab)}\n",
        "token2nrep = {value:index for index, value in enumerate(vocab)}\n",
        "\n",
        "## END SOLUTION Task 2.1\n",
        "\n",
        "assert nrep2token[token2nrep['a']]=='a'\n",
        "assert len(token2nrep) == len(vocab)\n",
        "\n",
        "for char in 'Elon':\n",
        "  nrep = token2nrep[char]\n",
        "  print(f\"{char} -> {nrep:3} -> {nrep2token[nrep]}\")\n",
        "\n",
        "def text2nrep(s):\n",
        "  return np.array([token2nrep[c] for c in s])\n",
        "\n",
        "def nrep2text(nrep):\n",
        "  return ''.join([nrep2token[n] for n in nrep])\n",
        "\n",
        "s=\"Elon\"\n",
        "print(f'\"{s}\" -> {text2nrep(s)} -> \"{nrep2text(text2nrep(s))}\"')  \n",
        "\n",
        "assert nrep2text(text2nrep(\"Elon\")) == \"Elon\"\n",
        "assert len(text2nrep(text)) == len(text)\n",
        "assert nrep2text(text2nrep(text)) == text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqvMwlSY-wU-"
      },
      "source": [
        "We use two comprehensions to build two dictionaries. One uses index as key and text value as value and the other one reversely. The assert makes sure that we can find the mapping and inverse mapping for each text character properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "2B8XDGPB6Noi"
      },
      "source": [
        "### Problem 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "d5792436-8735-4207-898d-ac35e926589e",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "9jmBjBR76Noj"
      },
      "source": [
        "### ®[30] Task: Text Dataset design\n",
        "The tf.data api provides a scalable way to to this. You need to:\n",
        "1. Create a `Dataset` of `text2nrep('text')` using the `from_tensor_slices` constuctor.\n",
        "2. Use the `window` method to configure a 'dataset of datasets', where each window returned is **always** of length `seq_length+1`. (see `drop_remanider`)\n",
        "3. Use `flat_map` and the provided `sub_to_batch` function to flatten the window dataset of datasets into a sequential data set containing sequential overlapping windows of text.\n",
        "4. Use `map` method, and the `split_input_target` method to split these sequences into appropriate (X input, y target) pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6REs7OKDAvt"
      },
      "source": [
        "def create_seq_data(corpus, text2nrep, seq_length):\n",
        "    def sub_to_batch(sub):\n",
        "        return sub.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(seq):\n",
        "        input_seq = seq[:-1]\n",
        "        target_seq = seq[1:]\n",
        "        return input_seq, target_seq\n",
        "\n",
        "    ## BEGIN SOLUTION -- 3 lines of code\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(text2nrep(corpus)).window(seq_length+1, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(sub_to_batch)\n",
        "    seq = dataset.map(split_input_target)\n",
        "    ## END SOLUTIONS\n",
        "    return seq\n",
        "\n",
        "dataset = create_seq_data(\"Trump is done.\", text2nrep, seq_length=3)\n",
        "for it in dataset.take(5):\n",
        "  print([nrep2text(it[0].numpy()), nrep2text(it[1].numpy())])\n",
        "print('...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5k2n4foDB0u"
      },
      "source": [
        "This creates sequence of data pairs as a form of tf dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "Ed1g173C6Noj"
      },
      "source": [
        "### Problem 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "cdd3dee1-4b87-4a40-8481-c22e24222bc7",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "vtqhMOMu6Noj"
      },
      "source": [
        "### ®[10] Task: Batching with `tf.data`\n",
        "Below is some code to shuffle and batch using `create_seq_data`. Answer the following in Prismia:\n",
        "1. How many batches of data are created per epoch? Is any data unused? Explain.\n",
        "2. More generally, add an explanation of what the parameters SEQ_LENGTH, BUFFER_SIZE, BATCH_SIZE are doing.\n",
        "3. Below we are shuffling and then batching. Explain what happens if instead, you shuffle after you batch. \n",
        "4. In general, when training, should you shuffle and then batch, or batch and then shuffle? \n",
        "\n",
        "\n",
        "hints: Use `nrep2text` to get a clearer picture of what is happening.  If you are still confused, try uncommenting the alternate `# test_text` variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_wfOFWsYf3d"
      },
      "source": [
        "# BEGIN Hint Solution\n",
        "for X, y in batched_dataset:\n",
        "  print(\"Batch: \")\n",
        "  for e in range(len(X)):\n",
        "      print(\"X= \" + nrep2text(X[e].numpy()), \" | y= \" + nrep2text(y[e].numpy()))\n",
        "# END Hint Solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayPgVMsOLE0g"
      },
      "source": [
        "1. There are 3 batches of data in per epoch. There are data unused since we set the drop_remainder = True, which drops the unused data.\n",
        "2. SEQ_LENGTH decides the input length of each trianing data point. BUFFER_SIZE decides the size of buffer from which we are sampling or picking. A BUFFER_SIZE of 2 means that each time an element is randomly picken from these buffer of 2 sequences and the picked one is replaced with the next element in the dataset. BATCH_SIZE decides the size of each batch in this epoch, we have 4 (sequences) data points in each batch in this case.\n",
        "3. If we batch before shuffle,  the elements of each batch are 4 consecutive elements from the input; if we shuffle before batch, they are randomly sampled from the input. \n",
        "4. We should shuffle before batch. If we batch first, the order within each batch is consecutive which will cause the model to overfitting into this order within the batch. If we shuffle from total input into each batch, we will not have this problem, the order of batch does not matter since the elements within the batch change every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "qFpt3bGS6Nok"
      },
      "source": [
        "### Problem 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "c5f71864-5800-405a-b430-d6c9ef90aca2",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "GGLjIWz-6Nok"
      },
      "source": [
        "### ®[10] Task Understanding Parameterized Models\n",
        "Add a comment for each parameter in the parameter dictionary above, paste a copy to Prismia as well. Your answer should look something like this:\n",
        "```null\n",
        "p = {\n",
        "     'EMBEDDING_dim':100     # short comment\n",
        "    ,'GRU_units':1024        # short comment\n",
        "    ,'LSTM_units':0          # ...\n",
        "    ,'VOCAB_size':vocab_size # \n",
        "    ,'BUFFER_size':1000      #\n",
        "    ,'SEQUENCE_length':100   #\n",
        "    ,'BATCH_size':32         #\n",
        "    ,'BATCH_per_epoch':100   #\n",
        "    ,'CORPUS_fraction':.01   #\n",
        "    }\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "prismiaParentId": "c5f71864-5800-405a-b430-d6c9ef90aca2",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "ZunV9T9C6Nok"
      },
      "source": [
        "p = {\n",
        "     'EMBEDDING_dim':100     # short comment\n",
        "    ,'GRU_units':1024        # short comment\n",
        "    ,'LSTM_units':0          # ...\n",
        "    ,'VOCAB_size':vocab_size # \n",
        "    ,'BUFFER_size':1000      #\n",
        "    ,'SEQUENCE_length':100   #\n",
        "    ,'BATCH_size':32         #\n",
        "    ,'BATCH_per_epoch':100   #\n",
        "    ,'CORPUS_fraction':.01   #\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUnQjyGwLWuk"
      },
      "source": [
        "p = {\n",
        "     'EMBEDDING_dim':100     # The output shape of the embedding layer\n",
        "    ,'GRU_units':1024        # dimensionality of output space of GRU layer\n",
        "    ,'LSTM_units':0          # dimensionality or length of the hidden state or the length of the activation vector passed on the next LSTM \n",
        "    ,'VOCAB_size':vocab_size # Take the size of VOCAB dictionary as the input shape of the embedding layer\n",
        "    ,'BUFFER_size':1000      # The size of buffer from which we sample from\n",
        "    ,'SEQUENCE_length':100   # The length of sequence for each input data point\n",
        "    ,'BATCH_size':32         # The size of batch for each train iteration\n",
        "    ,'BATCH_per_epoch':100   # The number of batches in a training epoch\n",
        "    ,'CORPUS_fraction':.01   # The fraction of thw corpus that we are taking to make the dataset\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "3kMyOtES6Nok"
      },
      "source": [
        "### Problem 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "46933baa-9db5-40d2-bd38-20be6fb22c62",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "IUQtBa9S6Nol"
      },
      "source": [
        "### ®[10] Task: [Tabula rasa](https://en.wikipedia.org/wiki/Tabula_rasa)\n",
        "Try running the cell below with the set_weights commented out and also uncommented.  Explain what you see in Prismia.  Include copy-and-paste examples of both runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWMU2iAFQGYW"
      },
      "source": [
        "If we do not set the weights, the generate text will each predicted character as a random pick from the VOCAB. If we use set_weights, the generate text will be the predictions from our model as the cloned model have the same weights as the weights trained in the previous model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "y8dgPOEa6Nol"
      },
      "source": [
        "### Problem 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaId": "bf5d032c-d9cd-4eb8-a4b0-cda829d8c539",
        "trusted": true,
        "editable": false,
        "deletable": false,
        "id": "zMpBffge6Nol"
      },
      "source": [
        "### ®[30] Task: Research Question\n",
        "The synthetic tweets results are very impressive when you first see them. But if you re-run  `generate_text` multiple times, the results start to seem very familiar.  Try and figure out why this is the case, then try and figure out a way to improve the results.  \n",
        "\n",
        "Document your efforts in the last Prismia problem.  Do not spend more than a couple of hours on this.  \n",
        "\n",
        "If you create some impressive or particularly funny output, please consider posting to the Piazza hw6 [Fake Tweet Fun](https://piazza.com/class/kjj6m8xbzbp141?cid=238) thread.  \n",
        "\n",
        "We are much more interested in you exploring the experimental setup and parameter approach available used in this notebook, than in creating a better solution to this toy problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d1uwEazRjbI"
      },
      "source": [
        "In the corpus that the model sees, different sequences starts with a few same fixed characters are rare. Therefore, given a start of a sequence, generate_text will have similar outputs as it is the only pattern that the model saw. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "prismiaHeader": true,
        "id": "bXjPtH916Nom"
      },
      "source": [
        "### Problem 9"
      ]
    }
  ]
}