{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 1"]},{"cell_type":"markdown","metadata":{"prismiaId":"0097b3a5-15d4-4172-9822-7cfcc31cceef","trusted":true,"editable":false,"deletable":false},"source":["# Introduction\nMany NLP models have HUGE numbers of parameters and are trained on VAST amounts of data. TopBot's [Leading NLP Language Models for 2020](https://www.topbots.com/leading-nlp-language-models-2020/) provides a small survey of some of the most popular ones.\n\nBecause of this, unless your working in a research group with substantial resources, it is unlikely you will be training your models from scratch. The data requirements also mean that these pre-trained models are generally non-specific to a particular problem domain or task.\n\nTwo key steps to building an effective NLP application are as follows:\n1. Figure out how to lever an existing Language Model to solve the problem at hand\n2. Improve the performance by judicious training\n\nIn this homework, you will do the following:\n\n- Begin by reviewing some NLP resources and tasks\n- Learn how to use pre-trained hugging face models for Casual Language Modelling and Masked Language Modelling.\n- Fine-tune a model to a particular corpus\n- After that, you'll use some Language Model sampling methods similar to beam search to admire your handiwork\n- We'll wrap up with some fun but important \"semantic geometry\" examples at the end of this assignment.\n\n\nPlease submit the results of this work to the [Prismia.chat](https://prismia.chat/projects/cba3b7ef-4b29-456d-985b-9f4bc5e495cb/edit-assignment/Prismia.chat) assignment using the same approach we have used before. Only copy code and results when asked or when they help support that narrative you create to demonstrate your learning and understanding.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"16a86ce3-a557-4c4e-9291-102d3624718c","trusted":true,"editable":false,"deletable":false},"source":["# NLP Resources and Project Data Resources\n**Q: There are so many different NLP tasks and resources. How can I learn about them and get started?**\n\nA: The Hugging Face [Task Summary](https://huggingface.co/transformers/task_summary.html) includes descriptions of many common NLP tasks and high-level PyTorch and Tensorflow based code for running each type of task. The Open in Colab menu button will allow you to open this page as a colab notebook using your PyTorch or the Tensorflow framework. [The Big Table of Tasks](https://huggingface.co/transformers/examples.html#the-big-table-of-tasks) contains a similar listing of everyday NLP tasks with additional resources and code samples.\n\n**Q: Are there any easy-to-use NLP datasets or other machine learning datasets that would be a good starting point for my project?**\n\nA: The [Hugging Face datasets](https://huggingface.co/datasets) and [Tensorflow datasets](https://www.tensorflow.org/datasets) repositories contain large amounts of ready-to-use NLP data that you can easily import. Another good place to look for various data, all in a standardized format, is the [Fast.ai datasets](https://course.fast.ai/datasets) repository. Kaggle competition data is also a great place to get started since it allows you to compare your performance with world-class data modelers. AI and ML challenges are another great data resource. These also provide a good frame of reference for your efforts. (If someone finds or compiles a nice catalog of AI and ML challenges these, please let us know so we can add a link to it.) **Note:** The non-Hugging Face dataset recommendations cover a lot more than NLP data.\n\n**Q: Isn't Hugging Face a PyTorch library? Do I need to learn PyTorch to use it?**\n\nA: No, while it is true PyTorch is currently the primary research framework used for NLP research, and many Hugging Face examples are PyTorch specific, there are Tensorflow versions of many of their high-level components. See the Hugging Face [Task Summary](https://huggingface.co/transformers/task_summary.html) for some examples. This [Medium article](https://towardsdatascience.com/tensorflow-and-transformers-df6fceaf57cc) from James Briggs talks a bit about this issue and provides a no-nonsense, step-by-step sentiment analysis example. It also includes links to relevant articles on tf dataset configuration and optimizer configuration.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"8353f87d-77d1-4285-8fcb-d84e81f8b70c","trusted":true,"editable":false,"deletable":false},"source":["### ®[10] Task: Your own table of common NLP Applications\nIn a cell below, create a table that enumerates each of the common NLP tasks listed in [The Big Table of Tasks](https://huggingface.co/transformers/examples.html#the-big-table-of-tasks) at HuggingFace.co.\n\n1. Your table should provide a brief description of each task in your own words.\n2. An example of each task.\n3. A specific dataset that would be appropriate for exploring three or more of these NLP tasks. Please include a link to a webpage that describes the dataset and the steps needed to download and begin to use it. Ideally, these steps would be appropriate for use in colab.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 2"]},{"cell_type":"markdown","metadata":{"prismiaId":"648d292a-329f-4b3e-9c14-abaf29d9eff5","trusted":true,"editable":false,"deletable":false},"source":["# Pretrained language models\nGPT2 is an example of a new high-performance Language Model. Please read this [article](https://openai.com/blog/better-language-models/) from its developers.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"1104c52e-9221-4063-86f7-f829207d1681","trusted":true,"editable":false,"deletable":false},"source":["### ®[5] Task: What is GPT2\nWrite a short description of the GPT2 model, in general. Include details on an interesting application or example. Be sure to cite the paper that originally introduced GPT and a reference for your application or example.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 3"]},{"cell_type":"markdown","metadata":{"prismiaId":"f0b3675b-ed91-4e5c-ad30-31dbe6769b05","trusted":true,"editable":false,"deletable":false},"source":["### ®[5] Task: Data and Society\nExplain issues and concerns associated with the widespread adoption of these models.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 4"]},{"cell_type":"markdown","metadata":{"prismiaId":"da0eeaf5-275c-4c4a-ab05-e9db86edb09c","trusted":true,"editable":false,"deletable":false},"source":["## Using pre-trained language models\nRun the Colab-Notebook associated with the [language-modeling] task in [The Big Table of Tasks], and work through it section by section.\n\nAdding these pip install commands at the beginning of the notebook will help you get started:\n\n```python\n! pip install -qq datasets\n! pip install -qq transformers\n```\n"]},{"cell_type":"code","metadata":{"prismiaParentId":"da0eeaf5-275c-4c4a-ab05-e9db86edb09c","trusted":true,"editable":false,"deletable":false},"source":["! pip install -qq datasets\n! pip install -qq transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"prismiaId":"f481c67b-acab-4521-89d7-c89ecdc520d1","trusted":true,"editable":false,"deletable":false},"source":["### ®[10] Task: Language Modelling 101\n1. Explain the difference between Casual Language Modelling and Masked Language Modelling.\n2. Why are the perplexity scores different between the two tasks as implemented in the notebook?\n3. Report and discuss the perplexity scores (using the fine-tuning validation data) pre-fine-tuning and post fine-tuning for both task types.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 5"]},{"cell_type":"markdown","metadata":{"prismiaId":"576fccb7-4d8b-493d-aad9-5e2232a679f0","trusted":true,"editable":false,"deletable":false},"source":["### ®[15] Task: Doing it your own way\n1. Update the notebook to use a different model and a different training dataset (your choice).\n2. Provide a summary of your results below. Include details on the autoencoder used, and other choices made.\n3. What auto-encoder is used?\n4. What fine-tuning training parameters did you use?\n5. Include before and after fine-tuning perplexities.\n6. Include example setups and outputs from each modelling task.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 6"]},{"cell_type":"markdown","metadata":{"prismiaId":"7132f138-3604-4e37-a5aa-8000fa49ff8f","trusted":true,"editable":false,"deletable":false},"source":["### Tokenization\nTensorFlow and HuggingFace.co both support several popular tokenization approaches. Modern subword tokenization strategies handle word stemming and new words out of vocabulary words in an elegant way. Lena Voita's [exposition](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#bpe) on Byte Pair Encoding (BPE) is a great way to get started. Extensions to BPE form the basis for [Byte-level BPE](https://arxiv.org/pdf/1909.03341.pdf) and [Word Piece](https://paperswithcode.com/method/wordpiece) tokenizers used for the GPT and Bert models. These allow efficient encoding for unicode characters and emojis, whereas many other popular tokenizers replace unknown characters and words with <unk> tokens.\n\nHugging face provides [support and clear summaries](https://huggingface.co/transformers/tokenizer_summary.html) for each of these tokenization approaches and several other cutting-edge approaches, including [Sentence Piece](https://paperswithcode.com/method/sentencepiece). If you are interested in working with languages that do not have a space between each word (like Chinese), you should read more about XLM and its generalization, SentencePiece.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"568d1037-e349-4754-85ac-8aea6a358e76","trusted":true,"editable":false,"deletable":false},"source":["### ®[5] Task: Pretrained model gotchas\n1. When working with pre-trained models, why do you need to use the same tokenizer that was originally used to create the pre-trained model? What would happen if you did not?\n2. In practice, word vectors are stored in an embedding layer in a neural network. Fine-tuning models with enough data improves performance. Explain why retraining word vectors may hurt our model if our training dataset is small and includes limited vocabulary.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 7"]},{"cell_type":"markdown","metadata":{"prismiaId":"4fefece4-3e38-4607-a9ef-ef0b9e00f751","trusted":true,"editable":false,"deletable":false},"source":["## Text Generation\nThus far in the course, we have seen examples of Monte-Carlo sampling, greedy search, and Beam Search to look at different model outputs. [The Big Table of Tasks](https://huggingface.co/transformers/examples.html#the-big-table-of-tasks) **text-generation** notebook introduces two other approaches.\n\nPlease open this notebook work through it cell by cell, and fiddle with it so that you can understand the behavior.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"b659bd3b-91b0-4c0b-87bf-82c46f4d0ccf","trusted":true,"editable":false,"deletable":false},"source":["### ®[15] Task: Greedy Stochastic Search\nIn your own words explain the following as clearly as you can\n1. The scheme introduced that references [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) and [Klein et al. (2017)](https://arxiv.org/abs/1701.02810)\n2. The method of [Fan et al. (2018)](https://arxiv.org/pdf/1805.04833.pdf)\n3. The final method attributed to [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)\n\nInclude the relevant Hugging Face API call. Include examples that are different from the ones given, +1 if they are amusing!\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 8"]},{"cell_type":"markdown","metadata":{"prismiaId":"9d3f2552-a8ab-4f6c-bd9f-0ed1ea6ae56c","trusted":true,"editable":false,"deletable":false},"source":["# Word-Embeddings\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"4b0543a9-4933-41d4-8da1-84043a866d9e","trusted":true,"editable":false,"deletable":false},"source":["In this part of the assignment, you explore the crazy world of semantic geometry.\n\nThe [Tensorflow Embedding Projector](https://projector.tensorflow.org/) allows one to project high-dimensional word embeddings into a lower-dimensional space.\n\n[The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings](https://doi.org/10.1177%2F0003122419877135) explore the word embedding models through time.\n"]},{"cell_type":"markdown","metadata":{"prismiaId":"9a643364-2e66-44a4-9a29-da379ebae962","trusted":true,"editable":false,"deletable":false},"source":["[Embedding Projector](https://projector.tensorflow.org/)\n### ®[10] Task: A manual for new projectionists\n1. In your own words, clearly explain each projection type that is supported.\n2. Explain the other controls and give an example of how to use each one.\n\nYou are welcome to use annotated screenshot (s) or other methods to do this concisely and efficiently.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 9"]},{"cell_type":"markdown","metadata":{"prismiaId":"bce4d17b-3be2-429b-9acb-5338c153a4ea","trusted":true,"editable":false,"deletable":false},"source":["[Embedding Projector](https://projector.tensorflow.org/)\nUse the Embedding Projector to find a [polysemous word](https://en.wikipedia.org/wiki/Polysemy) where similar words (according to cosine similarity) have multiple meanings.\n\nFor example, the word \"spring\" can refer to either \"flower\" and \"suspension.\" The word \"tie\" can have associations with both \"shirt\" and \"football.\"\n\nYou may need to try several polysemous word candidates before you find one that works.\n\n### ®[5] Task: Polysemous Word Hunt\n1. Please provide setup and output for polysemous word(s) you discover.\n2. For each set of words, describe the multiple meanings that occur.\n3. Why do you think some polysemous words don't work?\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 10"]},{"cell_type":"markdown","metadata":{"prismiaId":"77afa3bd-9ae7-444f-a52a-7c4647fa333f","trusted":true,"editable":false,"deletable":false},"source":["Embedding vectors have been shown to sometimes exhibit the ability to solve analogies.\n\nFor example, to solve the analogy, \"man is to king as woman is to what?\".\n\nThe \"man is to king\" relationship can be represented geometrically by the displacement vector between word man and king word embeddings.\n\n$\\displaystyle  v_{\\text{man}} + (v_{\\text{king}} − v_{\\text{man}}) = v_{\\text{king}}$\n \n\nThe analogous relationship can then often be found by adding this displacement to the woman word embedding\n $\\displaystyle v_{\\text{woman}} + (v_{\\text{king}} − v_{\\text{man}}) \\approx v_{queen}$\n ​ \n\nThe relationship is only approximate in that $v_{\\text{queen}}$ ​will be found nearby the point \n$$v_{\\text{woman}} + (v_{\\text{king}} − v_{\\text{man}})$ $\n\n### ®[5] Task: Word2Vec Analogies\nUsing an online tool (e.g.,  [http://bionlp-www.utu.fi/wv_demo/ ](http://bionlp-www.utu.fi/wv_demo/ ) or [https://lamyiowce.github.io/word2viz/](https://lamyiowce.github.io/word2viz/) ) or your own colab notebook and find three word embedding analogies that work. In your solution, please state the full analogy in the form **man:king :: woman:queen** and provide plots, diagrams and or calculations that support your assertion. If the analogy is complicated, explain why the analogy holds in one or two sentences.\n\nYou may have to try several analogies to find one that works!\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 11"]},{"cell_type":"markdown","metadata":{"prismiaId":"fc5095a0-1a81-4d43-a499-92f7c25f1400","trusted":true,"editable":false,"deletable":false},"source":["[The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings](https://doi.org/10.1177%2F0003122419877135)\n### ®[5] Task: Cultural Artifacts\nIdentify several biases that this paper explores.\n(Word-Embeddings)\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 12"]},{"cell_type":"markdown","metadata":{"prismiaId":"02fa8e41-3d36-40fe-8a96-6824db279054","trusted":true,"editable":false,"deletable":false},"source":["[The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings](https://doi.org/10.1177%2F0003122419877135)\n### ®[2] Task: OK, Boomer\nDescribe how one of these biases change over time.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 13"]},{"cell_type":"markdown","metadata":{"prismiaId":"c00573c6-080b-458e-8fb8-30d2b14aa86f","trusted":true,"editable":false,"deletable":false},"source":["### ®[3] Task: Unsupervised $=$ Un-biased?\nAn interviewer from the ChatBots-R-Us company asks you the following question during an interview, \"We know our models learn unintentional biases because we present them with a lot of customer features, in addition to customer dialog. We think using unsupervised learning techniques will help improve the situation. What do you think?\"\n\nPlease write-up you response in the cell below.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 14"]},{"cell_type":"markdown","metadata":{"prismiaId":"d1292489-a4ad-4442-a168-04be021de2d7","trusted":true,"editable":false,"deletable":false},"source":["### ®[2] Task: Axis Understanding\nUse the Embedding Projector \"Custom Option\" or another tool to create an illustration similar to one found in the paper.  Document that you have done this, and explain the setup.\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 15"]},{"cell_type":"markdown","metadata":{"prismiaId":"31f1a333-2f48-45c6-86a0-848a6c9de2cd","trusted":true,"editable":false,"deletable":false},"source":["### ®[3] Task: Embeddings Variation\nAlice and Bob have used different Word2Vec algorithm implementations to obtain word embeddings on the same corpus and vocabulary of words $V$.  \n\nIn particular, for every word $w$, Alice has obtained ‘context’ vectors $u^A_w$ and ‘center’ vectors $v^A_w$, and Bob has obtained ‘context’ vectors $u^B_w$ and ‘center’ vectors $v^B_w$ for every word. \n\nSuppose that, for every pair of words $w , w'\\in V$, the inner product is the same in both Alice and Bob’s model: \n\n$(u^A_w)^Tv^A_{w'}= (u ^B_w)^T v^B_{w'}$\n\nDoes it follow that, for every word $w \\in V, v^A_w = v^B_w$ ?  Why or why not?\n"]},{"cell_type":"markdown","metadata":{"prismiaHeader":true},"source":["### Problem 16"]}]}